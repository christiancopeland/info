--
-- PostgreSQL database dump
--

-- Dumped from database version 14.15 (Ubuntu 14.15-1.pgdg22.04+1)
-- Dumped by pg_dump version 17.2 (Ubuntu 17.2-1.pgdg22.04+1)

-- Started on 2025-01-19 18:00:14 EST

SET statement_timeout = 0;
SET lock_timeout = 0;
SET idle_in_transaction_session_timeout = 0;
SET transaction_timeout = 0;
SET client_encoding = 'UTF8';
SET standard_conforming_strings = on;
SELECT pg_catalog.set_config('search_path', '', false);
SET check_function_bodies = false;
SET xmloption = content;
SET client_min_messages = warning;
SET row_security = off;

--
-- TOC entry 4 (class 2615 OID 2200)
-- Name: public; Type: SCHEMA; Schema: -; Owner: postgres
--

-- *not* creating schema, since initdb creates it


ALTER SCHEMA public OWNER TO postgres;

SET default_tablespace = '';

SET default_table_access_method = heap;

--
-- TOC entry 211 (class 1259 OID 16410)
-- Name: documents; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.documents (
    document_id uuid NOT NULL,
    folder_id uuid,
    filename character varying(255) NOT NULL,
    file_type character varying(50) NOT NULL,
    upload_date timestamp with time zone,
    processing_status character varying(50) NOT NULL,
    doc_metadata jsonb,
    file_size bigint,
    hash_id character varying(64),
    qdrant_chunks character varying[],
    raw_content text
);


ALTER TABLE public.documents OWNER TO postgres;

--
-- TOC entry 210 (class 1259 OID 16393)
-- Name: project_folders; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.project_folders (
    folder_id uuid NOT NULL,
    project_id uuid,
    parent_folder_id uuid,
    name character varying(255) NOT NULL,
    created_at timestamp with time zone,
    path_array uuid[] NOT NULL,
    updated_at timestamp with time zone DEFAULT CURRENT_TIMESTAMP
);


ALTER TABLE public.project_folders OWNER TO postgres;

--
-- TOC entry 209 (class 1259 OID 16386)
-- Name: research_projects; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.research_projects (
    project_id uuid NOT NULL,
    name character varying(255) NOT NULL,
    description character varying,
    created_at timestamp with time zone,
    updated_at timestamp with time zone,
    owner_id character varying NOT NULL,
    status character varying(50),
    settings jsonb
);


ALTER TABLE public.research_projects OWNER TO postgres;

--
-- TOC entry 212 (class 1259 OID 16422)
-- Name: users; Type: TABLE; Schema: public; Owner: postgres
--

CREATE TABLE public.users (
    user_id uuid NOT NULL,
    email character varying NOT NULL,
    password_hash character varying NOT NULL,
    created_at timestamp without time zone DEFAULT now(),
    updated_at timestamp without time zone DEFAULT now(),
    openai_api_key character varying
);


ALTER TABLE public.users OWNER TO postgres;

--
-- TOC entry 3373 (class 0 OID 16410)
-- Dependencies: 211
-- Data for Name: documents; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.documents (document_id, folder_id, filename, file_type, upload_date, processing_status, doc_metadata, file_size, hash_id, qdrant_chunks, raw_content) FROM stdin;
241b9157-720f-4f2e-b0ed-80172860a774	\N	2501.01342v1.pdf	pdf	2025-01-12 12:56:40.424014-05	processed	{"title": "", "author": "", "num_pages": 10, "creation_date": "D:20250103023108Z"}	3203528	\N	{e33f91f1-9b9c-4883-89b6-74a8fa6eda8e,c3b03421-e336-4831-ab3e-1d93e757b64d,439b295b-f1b3-4944-bb4e-21cd1b8152bf,74733b96-7128-4e91-8b21-542fdb2b9f49,42c7fc0d-f9ff-4347-a25b-539d91e7b4af,290207ed-532e-4950-bfaf-b55792e92738,bb2f084e-b498-4d87-bfb5-a6a27e8a1a36,dc9de91f-7c1b-45af-8443-0ac856ebb605,68471d8d-ab59-4bfa-9f7b-f676f5bb3d9b,8fe1d362-b462-47aa-aeab-38b9f566bf0a,1ca82eeb-d1af-4a83-905c-fc85964c92d4,c1afe04e-dd6e-4353-81d3-9cac5d0d988b,d22cd6b0-8af9-4216-a3fb-05c06083af96,4d98a3c0-97cb-4e2b-9f06-0d237456bba9,5644faea-4fa0-4dd4-92c9-ed5c24f0aeaa,a863a1a1-0489-44f7-97d6-e6ba74e9d72f,0611db59-88aa-4576-850c-9c98b88cf832,eedfba8e-a904-4920-87e4-43ede7dd7c71,b1f646ce-4773-44fe-856f-5416fbf7a507,643d343d-5c77-40d5-a2a1-691dbdb983ee,1fea361f-4bca-43c3-bcf3-677593abb1ad,cb56f17f-c2a9-4052-b338-deef0eb8c4f3,ac97b3fd-e17d-44e1-9d3a-4a99d6551d5f,906f7ee8-cb9f-446b-a209-0022f2332f8e,37d58f23-9f86-4718-a820-85d9f5df6cd8,692b6b87-8d3f-4446-ac32-6cce326c70a2,8f68734b-3da0-46f6-847c-cbc9f5445d96,99a39c30-eaf7-4ef5-9f87-d6dec4c2433f,95158c2c-d237-4a83-920b-cf5a6545db76,1899a3bf-7326-40b9-a484-eca7cf1acfb9,e47044cd-1591-4812-b908-b59266757fae,503253c0-2ae6-453b-81ec-ebb27af3be72,c8886412-6aca-478a-aed6-297ff57bace1,ed9f273a-0657-4551-bbe2-c1b5dd181ab7,47901b29-6713-4801-b8e0-e446f0c5a386,0c01955e-8138-43cf-a800-5ec014cada9c,9bfd3f27-0788-4571-8ffa-f9feac80e44f,fe217a38-c5aa-4e25-a2fb-474f25685615,8d9cd90b-0368-4ff5-b7ef-3196c8f848bf,e6c8f744-630b-4b15-a46b-665d49238369,3258448a-5677-40dd-baf0-ac3d823390cd,3464420c-9d7a-44e3-a9ab-79922f471319,a1adc156-cf9a-43db-999d-0ca25a332bee,881acf8d-6f8d-44e8-b02b-cfd4e65c8503,4793db6d-ac6d-436b-b61a-10a7c8b46584,f7d7c75f-ce42-453b-894f-ba74ea1beac0,454c3afd-4dc1-493c-a03e-feac550f3daa,ce3dc9f6-a8fe-4797-91be-f8d1558fca8a,c7d66012-0f09-4eea-bf6d-a8101f2743a8,8ce2f480-f142-4704-8ce3-5884eeb87b40,b41fc8a3-24be-455a-890a-aee14fa069a0,e579e912-b8dd-4811-a7c5-ee3b480efd7c,ff0aeb51-9090-4384-90d1-e136ca0556a5,b2a101c9-bbaa-4fb0-b8fd-13578d8f7cdb,a15a1bb7-0d05-47f7-86bf-6b421afbc4f3,5e64d5ab-767c-437d-9c87-c0808c75e521,bf009c8c-1666-42f2-8b64-cabb7945a79b,96c1d176-56e7-410e-9ebd-cadd4380a3b4,a9a39e45-21b5-44ae-9a3a-a9d039768814,99ed3bb7-0fc0-46fa-a5da-f3d4a70ec692,6ad7fd1b-65fc-4d24-b43e-04b7522db9ea,5b260539-6c02-4400-bf46-33c0ff3c4109,30e119e7-4e9c-44d3-b3a2-7dd24d3bc5a8,b8836593-8eb1-419e-857f-87f890c913cc,a9cdfa35-0085-4a43-bb5f-2681b2ecd7b5,d9c6460b-e83d-440e-b46e-1be5ed998358,f9ebdc2b-519b-4b78-b1a8-c0382d293005,9e747296-fe14-426b-be98-709cb1091b31,00cf55e1-4159-456e-9eb4-dfafb9c5cc0f,104f17aa-dcf5-4a04-8a23-5387c8032300,3d9ce991-c468-45f7-9112-a5af79f2cc43,36246bb6-b2e9-4b8e-a404-0607fd89adae,0d531b24-dedd-4c9a-9745-496e953892ee}	\N
ca88f6c5-dca1-49e8-8c10-de1219dc73ba	\N	2501.01342v1.pdf	pdf	2025-01-12 13:06:32.60169-05	processed	{"title": "", "author": "", "num_pages": 10, "creation_date": "D:20250103023108Z"}	3203528	\N	{5a18617b-0c89-40a7-8b70-b10522935b25,2ba7cab7-b667-4578-ad42-e9a759c87f7f,8c107012-645d-43c3-b97d-b7fffbc42a05,fd6162e8-e31c-421f-88c3-101b040a4ef3,7fd28442-b831-4d17-bdfe-c1e1107cc083,baf998f0-0a76-4adc-82bc-cd220fe418fd,210794e2-5057-4250-aa35-0bbb97894aca,096fc355-dcee-4348-8bd5-ba30695404b6,0ed0008d-cd8e-44a7-bc94-b555237f24e8,7bef2bad-fdcd-4977-80e0-91ba2149aeab,7cd1aee5-6981-4e2b-89cd-a4b68458cc65,ffb83885-592b-43c3-bbbd-2b13ef095004,fe0c190b-0a0e-4fc5-bc0a-afa8bb029b74,d0de122b-6334-45c5-8a29-4bc304f16b04,8c40a63d-4be6-4149-8520-3d5a59e3743d,688f0895-5987-43a6-a187-17f674e69181,7ad958d3-78cc-4364-87ff-ce6fc29e70f3,f20fbb3d-f3f0-4fd1-b9f4-6d768539651f,b136b048-afe6-4099-b3c8-30765a39a9ff,16876516-1d9f-4d14-93c1-a1a0298c1d69,59785525-4d7c-47ab-be2a-94b4fd294657,65645139-6a3d-485a-a5ce-587b2ea5f897,a0b81a56-19db-484f-bc7a-292690e59b5f,31d4e893-b94e-4149-a094-097d73f6ff4d,f572011e-ae0f-46ab-a85f-fe36539a293e,44f5482d-d347-4148-b0de-1ac2c07c50c8,6deb23cc-ac85-4ba3-b01f-6aec606f1d26,f8d6d2d6-f580-4610-9e11-2964d8ce0ac9,e9dc0a38-a43e-48c7-a365-c52bef61d679,70364074-9a50-4fbb-971b-462911da1fa3,024ed3a6-9b9a-478b-a152-5f53b194e7f5,09e3f1d3-4af0-4ca6-b14d-f67092190861,239370a9-a72a-465f-88e6-b0bec6cdb632,745533f1-8006-492a-be75-f5ac614f4073,8d9e6548-b3e0-4c16-a2e6-48530449b3e6,759f5a34-93a1-4535-a57b-152e33430b34,c5efbc82-3020-4ff6-8125-d170e30385ce,c6b82522-eb01-4b66-a273-a5fa8a534ba1,ea950485-9810-4dc0-bd87-1b0c54d78704,95990836-d7a0-4ec5-8b0f-6fe003872749,71f36a49-2373-4b97-9590-612889524279,837eb2c5-7fbc-4981-865a-edcf3c00eb96,49b8eed4-c08e-4eae-8507-47384a46316a,85f3c0f0-ede6-4641-8b12-1240ac0bad38,54298dd5-3ffd-4624-8b73-4d08f4fc4519,53a134b6-5e5a-4a29-9337-b6a49d7f161a,ea8c2acf-0bb1-4920-8f11-fee84442e0f8,cccdf645-32fc-46f8-9db9-f1a943603090,d899ee10-576c-4c04-bad4-0e7572b96efa,3b13c3e4-82df-467d-955d-d8d1a091d84b,b08594c0-19be-42ec-af4c-128576d1f2b5,db08dc7f-2d0f-4a30-a901-de1c6e569b8c,a7d0072c-528a-486b-8053-30049796c677,91ef6fbb-ab0d-4263-8764-e39aa5004bf9,f238db9a-d1c0-48bb-9aee-d551cfbd2214,5544fcf2-9e95-4c0f-ab97-5e608734f6b6,f8827ca2-61bb-4d4e-af99-29f2fd26ccaf,68956f7e-058a-454c-a15d-e2ddff7250d9,ece6b59e-3dd4-4afb-bc8b-35faf484816b,ff1a01bf-7472-4f33-8870-9c42e7836ea7,d028e1fa-40be-4c1a-9aef-a70b4b730298,492a5685-a1d6-487a-9158-55c442108f10,5e5ec0e0-3aaf-47db-9367-31a0178d0fef,fbecb093-615c-4f14-9515-dc41ff37bff3,98a30a21-fa9f-4a0c-8c8b-bbcb68de98f7,f9011f61-5f5a-4f8d-aa4b-751b108be525,c28d78d2-6097-4958-9f76-52b0433f2bd8,c24a9b22-1d38-4698-b324-3665fc209963,d60b0123-3b60-4620-b740-57660ecb6fa4,d1a958e7-363a-435b-8581-17776cd0476f,7f305fc9-64c3-4de9-9d52-9db186d7a0ed,614bc809-23bc-4f87-9683-ca09d816929c,1f9f36aa-2486-411e-a3d2-df8059c18f23}	\N
e7466538-68a7-4e7d-ba28-39468f4ec917	1572e013-97f3-4dc6-b860-572cf420f8dc	2501.01342v1.pdf	pdf	2025-01-16 16:22:56.166624-05	processed	{"title": "", "author": "", "num_pages": 10, "creation_date": "D:20250103023108Z"}	3203528	\N	{0e5cf397-46a1-46c2-b8c4-03c5003ffb0b,62d8680e-271f-40bc-8ce7-7b7d14b4abb9,9c7e8f9a-7163-4dc1-bfb4-07f1d992795f,0fb5e79d-6d03-435a-a7ba-0a885fc78bcf,7278008d-79a6-4b48-b316-a87b34a4eb04,a36d14e2-3eca-49d0-9c94-58f49ecec807,f9fe3e4e-dc96-4758-9897-83fa1d28aba9,63b28730-bbbf-4d4c-be0e-a74d5894821c,66b16efb-dece-4c92-bb4b-55afcc91af81,1861557c-94ce-4a95-adc8-1645ba4d214d,15a5c678-08fd-4226-8fee-82832c79c802,79b62f77-e1ba-466f-8a35-cfe4109253bb,90fcb4c4-f7e8-4422-9c73-4b62e1101e06,fce221ba-77b2-44a7-a3c7-36e1da601f14,59f8cddb-261d-47e2-a567-499170295c7e,85b7fb69-be13-4371-b318-98f99ab89bed,5bffb971-c980-45ad-b0ad-1634a4da359b,bd2ae739-a560-4eb7-9e40-1a38ec1e8a71,bf7ebdaa-1d0d-4b9f-afc8-a3d8ac5493a3,e2875d2a-7378-47e7-ab8c-e78111f5cb79,d80689ed-a301-4359-9ba4-78ad661113dc,0e5b7899-b11e-463d-9deb-1c3b1f4c0828,1d17ba52-d94f-403a-b36a-3f65b9d1206b,03e56193-bd18-45e9-ab96-86f7d28a9ba9,e3ba9e64-fc9e-41d9-8631-3b0dbc66ded8,4d6a0590-dae8-426e-a875-977e1f30feac,6478e84d-53c6-49c9-a1b0-7eb12535ba5f,96a50eaf-fa46-4cfe-a961-777fd2136adb,7452625e-c140-43f7-ad33-fda31e9375bd,4df854ca-b1f6-4d32-ba36-ccab77927d3f,34828245-f4eb-48d7-8c69-4b428acda2f3,cfdd1197-a02c-4d15-954a-6634c47f17bf,ecb65ba1-adcb-40cf-a9fa-67c4527e028a,0efda07e-4bc7-44f2-bab0-3467ea9d5886,128e34c3-a951-47be-9c24-78fe53cdc946,b2efbb90-26f3-4b73-93d3-08a6d7112419,61423d3c-2ffe-415a-9174-cb6fc8e670b7,42a1f587-60f8-48c7-8256-331cb194a253,62a7f5f0-dd14-4199-bd0e-f5d9869d320c,a95a66f2-82aa-4ebe-8328-65336aa87636,386caec6-479e-4317-a7c6-9992d3dab95d,60e36997-40f7-4375-8024-5fcf4b08ccd1,93315f8e-17b6-421f-9b7d-a6d9726004ef,3072d118-f5f6-4036-90d3-b94f97b53fba,77f777ad-fa8d-4e6f-a21c-5513ad634ed7,88f211f0-a7c0-4170-88af-cafe564e12b4,6e93fd4c-1000-491d-b394-428dab2d3597,df5bce94-1388-4240-9ee5-e1fb325f9231,c40901e6-f9c4-4c40-86bd-36ab7cb62800,d16dd914-4791-45f3-a2de-6c5b93ff00fd,0af5a103-8e15-4261-beb1-9e01d22ccdae,91c5e976-54f3-482b-9b14-e5dc2066239f,aef657ad-191a-4e87-8251-0b34709a8bdd,519f3f09-b58e-40e4-8e74-c92f0c72a769,ca7faceb-e780-4e25-88a2-acf29d1e4ac8,1c2bd346-c6ce-402c-aa8c-6b7c53ae9012,c2aab896-0bf6-448e-8253-27be8458e90e,f3592661-6a76-4055-ab43-80eb1c9b75e3,8e5c7884-64a9-4b47-acc0-cf340975db41,884388ea-bdbe-41c1-ac4c-904451ffdd05,7d8971e2-8aba-4e9f-a319-128a58de51be,188f539e-54a6-40fc-92e3-652160fa3b53,24a48f57-d389-404c-9c19-9e3c246ea802,e2df562d-a6a3-471d-a90d-52d3ae734335,f1dfd721-9a74-4478-98c0-5fec9082861a,59a2bdef-8c74-4a94-a2ca-e59891fda4f9,52e6c151-2b60-4ad2-9eb6-46da5f499b20,7e3aa8b8-6470-4c2b-949c-e6cca284d2ba,83db465a-0ea1-4c03-8d47-22690ba52f5e,e704d4a9-de17-4c27-894b-2c326fa05758,d69f97c0-bc6f-4e66-9a3a-dbeada7c2f2f,6bcf6a36-dd36-490d-8734-b3883c755d33,22fd296a-36aa-4e69-adb6-08a0b811c752,5ca455ad-5922-40cd-9519-fcb0a1b9fbb2,f2dfa29f-a11f-4604-99f6-04a6d1e98292,b4c333fa-1c45-44a9-a6b1-a5d5125723e9,70a70271-5fa0-4632-a3f4-48f6eb99eb30,7bc5ee0f-91af-4aac-ab84-aa661b41bff7,462e79ae-abd9-4903-9686-d4d0db3db544,8d1c79be-79a8-43c1-a15b-0e7582affe69,f6e9aca9-9c6b-4741-ad9a-6d911369be3a,de1be8c3-ce56-4f3c-aa71-f7465b259238,c8553743-12d7-42ed-b8f6-444a63082785,f222e0b6-3b68-4dfb-a320-d0cb98a497b9,a914f27b-33ca-47f6-b2a6-a85e6c1ed986,94969082-0b1b-4619-891a-243a5326e344,578a9bda-8b79-428e-8ac5-727e5ba75ac2,8e8aab09-034f-4512-b94b-a9b9c37ffd91,1acbca78-10f1-4420-9a66-d9d0b8f9d516,904db48c-e5e1-4bd2-bd61-ea6f702334bb,6f79e930-62aa-4c2b-9501-77587915945e,2381b527-20aa-406d-bc37-d7efe2441955,dcbd0596-b6ee-464b-8c78-a9fe345ae2d5,e777e23a-caef-4a37-987b-b3f731d650cc,03ad5e7a-be66-4a66-af0e-cb4e8c097660,f93d90b3-2199-4b89-95f9-c3b72bacc5b1,d0a9d7d4-eea3-4e17-8ba2-818a8f8064a1,56539fd9-289a-4a60-bc8c-501c7f170a3f,d3c823c7-d9ac-4a76-8e53-a7af5211bc28,64b5ad86-993c-4169-9dc6-9db86e558259,0697c98d-20d6-4132-a7e9-f7a54adeaabb,9b54dc30-3f50-4a48-9050-25ac25556db4,7285a9a4-7a43-436a-8095-ccef18774ff9,4d1fafc4-4b52-431c-8903-59a9cc36fb2e,61024cba-82d2-4255-a48d-f8657a8f97bd,474052ba-9283-4855-8ee7-63b986f4696c,eb906e5c-2314-4de2-89fa-4b59ef6c1bf8,ed6a6b57-1593-46a5-a86a-3e0964356bde,769c9e0f-8d2a-48c6-acd2-ed8c48339aef,fa3bf06c-d87c-47e3-bad0-e01b0596aa6c,6cbbb672-fba2-454c-a2b8-55e146d18124,aaab0fbb-af12-47aa-ae48-9e3b665fcabd,84820f45-4c57-4177-b8cd-40198a3c1f86,8b6a58af-53f8-4f0e-8df2-bc061c4c4201,e832527e-b354-47d7-be14-672113965144,58026e09-53d2-4900-8ac6-4dc4aca503a1,3eaf4277-38f5-4fb9-98fe-3de7be81af63,64d8cf97-bb50-4343-a06f-3fb27a0a2c33,301ed772-67e1-47a6-a810-82c318f7f742,edac3287-95e8-4cc5-a693-2f7a64bb707f,3e8e9a75-1534-4595-85cb-c0365f6a52d9,4ae49a0e-4906-4423-be7b-4a017bb24761,adf1bc94-0879-4775-839d-4cf56526b09c,73c5cc86-37f7-4d52-b559-1b8664629081,c7b6e27b-4c2d-47cb-b410-795d064c50f9,8b7d36d1-aed0-4435-a636-54f691881b5f,56b7ca37-fab6-470c-9252-a744bd75dd59,302b7db2-4dca-4a04-9b81-3f5aba256b24,c8af06bb-9bd9-468e-a9be-d438bac5ab0a,82213e3c-a47d-4026-9700-9ee3eebc1584,414942f2-c98a-4104-beee-b1b2767b4b4f,03df253f-9a18-4e21-ae6a-c7efac897958,12fd684a-2237-4e1e-ac11-0e79f3e38433,86a4af33-a0f5-4e4b-8b09-eb2e47f8136c,85cb3f26-58e3-4e02-80e5-6471ad085e2e,ee4dcaa1-8e3c-429b-a068-308b495fbb8a,158f0056-ecea-4ea7-ac10-4da322da9a6a,a0bd67f6-6139-4cd7-9850-d6e06a6fa2f7,f0e0ef76-a5de-400e-8388-706e2cfcae1a,0914b387-55b9-4f1c-9a6c-839af174c97e,2c4977fe-d004-4c85-8ec1-4713c7731ef6,984d36f0-5e1a-4339-a587-73f452a6ef50,7809cc49-90d2-4342-8cdd-0947e5a88f5d,157b65f0-9aff-4433-949a-c9fc398d6a58,6216f91b-1e45-42f6-86a9-55e5b6889771,e73de184-7cd9-4d70-8d7d-7448037c5855,caf68cb6-9e4a-4468-a42e-46711749093c,beb1d093-fba6-491e-b1f8-2aed5478f897,bb4eb88a-4f73-41ab-8234-27f7bf8f0ff7,bdb1c698-f754-4d44-aefb-2b0bffcd102d,e695f984-c354-482e-aabd-eca9a0d86496,daf71dad-7695-4137-967d-8bbab0fa6bed,ec18e864-6303-4be2-b38d-473b98fbcebf,9c75b2dd-c7f0-4b5a-ae9c-c4cc2c511321,eb9682e6-2313-44aa-ac09-b38b6f213314,322000e1-a0d2-4c8c-bfdf-64a3c9680acf,425dcf2d-9a2c-472a-bb6e-bf06236a5c18,ec6473df-e467-409a-af51-b6c3d1c321e5,bf254da5-73ca-466a-a8b7-768dd1670f51,2cfde94b-b310-4e1f-a28b-b0353efd2a94,2c5fa7d2-6e3c-432d-9aea-58fa1d76d40e,e62b2b75-cdc5-43ca-b91b-685c6b69fbea,7cb99cb1-92e8-4101-aae9-97f333a709c8,14fa8777-2e18-417a-a726-78abdebc7b4a,19ea7d23-5ce4-4d4d-a2eb-9b45f45221e9,dc1d5fbd-3438-4af8-950f-0ca7fdc49dfd,f9cc70db-0ff2-4e2c-a508-6c7aea6003b1,d9cabacf-a739-472b-930c-d03b28a8adc9,5d27f467-d967-4c38-8c89-6d1185f5a679,6a69fb53-89e5-4819-a249-428f4a91283d,e14c0ba5-ffd5-45a2-b231-c7117af79df2,e8ea6992-fae1-424e-b6be-f49115dd2f49,80d09075-6f3f-47cb-8565-28dc2002a760,35619cb4-afef-49a2-857f-9186c4a15bc8,351ab8b6-2e59-4b6f-8908-5301a8de6bce,8294a3c9-d3c6-47f5-b3d8-b5e3f3c6fbcc,c0a1877a-7df4-4966-9d97-fbfcb8945165,f42845fe-47e4-4d8b-8c75-ec0caad6fc97,b7dfc5e1-51e4-4bab-9f87-4581fdc6bd86,1fc4a046-4091-4b31-953e-be1c09a81f7d,40f4bb45-ff9a-41d0-8a62-e26526aea699,097a7c54-1a3b-4695-a0cf-1b59c6a333b4,6c1060c8-d808-4c62-b4eb-79717bee0445,f365aa7f-bfc7-4a48-808a-1f77b2bc33e6,8adb5e52-bc14-451e-82f2-0f1ae4d3f8d4,ea0f6fae-1526-4c2d-91cd-ee8e25030535,8e6f00f1-e722-4d7e-8b8e-86b5f3140ea5,2d740e89-5cdf-40f6-ab0f-41cd1732533b,0dacb9cf-826f-4787-b19e-1ac5fc29a269,8bba78ce-527b-4846-9c3f-41e8192195a0,3e608704-98fc-4337-b654-43a819d523f1,ee2ffab6-78f8-45b0-9bfa-3a30cafeb824,65798de2-f1c2-4467-9ea0-007f4073903a,f6476d76-7331-4bb6-b9e8-1b3f3bdece71,4ed849e4-19a4-48e9-ad02-02792493b2db,aa15fc68-b6a6-4b3c-8549-a73e1ce74e7b,b6fd8560-e14d-4d0b-8099-b1ba8f086a94,c57a1c68-9e5f-401f-b567-de42d3eeb269,38a27f68-37c8-49f4-9db6-5c262ad55f89,8d5bd076-e8ef-4c56-a83c-bf5c180f3485,65722014-7176-47bc-9b9f-22df60ec3af1,014b0b16-e524-45b4-8651-3b360aea3649,3c8e5d43-e230-48e4-82f2-ccdbbbdf917e,8df1557d-5717-4a7b-a4e9-b78744680e6e,def489db-8843-46e1-a4c4-042a7bd9fbdf,e25b49aa-a70a-421f-ba3c-b1aaedd31c66,b7867621-f9d1-4fd9-9e42-3cb73ab9ec10,6ea8cc37-52ee-4783-a68a-57fa16e998cb,d1f58268-3ce2-41d3-9258-bbdaf5c0fcf8,14ee70a6-ecd2-4be5-8f7d-8692d26f3a44,7e8cf978-449f-43f6-9002-58c12f878bf4,184acfe3-d553-4f11-8812-b0417746921d,78f70cdb-01a1-4c20-82e5-99102dd695c4,0d048a9e-4261-488f-aae6-728e629dd7c5,28c1cd41-c35a-477b-a740-2edbb2c98ddb,edca1081-acb8-466a-b924-875f531b67cd,aa386116-1e6a-4822-8f1d-732aeccba793,17fd5d04-5600-4d4a-8052-966267f50b2a,7e0195df-811c-46a3-85f7-01b424e0fcca,7fae04fc-18e9-489f-8c22-cba98a8609be,cd680ccd-7101-41ce-acdb-530a3d3e4e98,fedee462-1efe-4291-9bd2-acd8a5779a73,da9e645b-058c-4a95-9b0b-e4816d205dc4,cebbb5ad-b799-4939-863a-d1385c974d23,7a4866ba-2a52-4704-b1f5-60e9107f5535,cb01765f-8e43-4351-bf87-382d183ca6f1,e356d58a-f144-4ea1-8305-ef8205d9ad67,22db179c-96a8-4082-bf54-adc6478e91f1,7c819aad-0f35-4f68-8020-820731234651,c117dcaf-a21b-415b-b48e-65b5854baa5f,d5d78686-77ae-40ed-b9bb-386ea292613d,cf492570-d5ce-4886-bb91-c589b332ea9d,4b03db4e-f345-40d6-aea7-b41b5ca83850,5ed16d52-2291-487d-b1a1-78d94a886912,a6ca776b-15c9-4f1c-884e-3c973ca6f341,5f10cf73-944f-4d9f-a746-4820b670ff77,82d88ce6-a166-4c19-9323-e34a48d3fc4c,90b30427-8027-4786-a5da-321b4757dac0,043f89a3-c564-44d7-b045-9994b5f9999b,36688d47-f96f-411a-8af6-985776802191,6a2bec42-d259-48fa-91b0-156edebb1ddb,a5cf4fe8-8fc3-40f2-b97b-6de9fcf7aa86,0d3f9709-d4c6-4205-b21c-fbd8407c3024,415f3b26-b7a3-4ca5-8e38-3d00f8a2e51d,04003848-7c59-4b6c-9d3d-26f0a2a394d4,85764d8a-7ef3-4a43-9194-a0425a10a7c9,09e7f95e-2889-443c-bb9f-313d1ec7f6bc,a0c70f78-aa3c-42f0-a861-7706b151d313,b864a445-3f6b-45fa-a668-1e98da52d124,30074ef7-6e68-456a-bb21-7f5befddb9e9,72244424-9fb2-4343-b660-6be68dcacfd8,04f9c89a-13aa-4724-adca-33616a4e63ab,c61a7c9a-0f70-4a43-a197-82b733f0c186,bd63a649-ba4f-4005-948c-a14a49ea7513,b5dd097d-0861-4f7b-aa4d-23326c6ae99f,98e78fda-7355-49f5-bd1d-d9718cd9fd61,c37f410f-7b42-47bd-90e9-ae3c96f95bdb,4369c312-97a8-4275-b7f0-fef226d1874d,9a0884e1-440a-4a13-993a-1929eb09b3c1,8bf4cec4-f179-4260-bae8-7a6863c2deef,9f28dcdd-028e-407f-8f18-e48cf9cb0533,b8b51f49-d9eb-499c-bb1a-f58c393558ee,c3bb7447-3c62-464f-8c63-1130d79b693a,0af9d8b7-a1ca-4958-906b-4fbbdd3cfbc3,a5968f99-f01d-4844-a6fe-3d865aaeae1b,22229c9e-7d59-4fe2-be22-08c00d154de6,9bee917c-a05d-41c7-bc38-67804c82d2c3,bb044aba-d9a9-482b-8360-4807da81cec7,94d5d642-5134-4687-a07e-234e541a8492,edf04e74-7192-4695-a383-5d08fb93ebc0,f2d52b91-1b3d-4b38-9117-3c309d418a28,2887ec66-d4bb-4b20-9fd3-33aac7a3fff0,17d61bc6-477e-4adf-8e4e-e68680819a70,7da663ce-962a-4119-8759-3019752b88a1,1e996523-0531-46e6-9318-051ef3835967,1153c482-9eb4-43cf-8100-d85ac5274b2c,0b7fbf1e-9c02-475d-9376-591fe7680c8c,13ae9af6-794e-42a9-9750-779c63fe9d86,33986095-f37d-4287-b13b-429508173368,ad2898bb-c9d4-4526-98db-cd08ddaf066f,fb5feae2-ca63-4738-93b9-844ece92fb75,04d3e1de-5fab-47b2-8484-db2a35795e7e,c743a23f-431c-418b-b9c7-ba648e439678,976e1133-2500-46c0-83cd-fc785be023f3,d472da07-1d63-4dfa-addf-38eb765b329a,3cc4672a-5798-4ba0-9f5e-7e989d2aaad4,0ebb8f56-a93b-4ac8-a1a4-e8517fd55ddc,54d63e38-e3cd-4931-a56f-77a6ec8839da,76fd91ed-befa-48a6-925d-ea4f70056fdb,525ae6a3-27d1-4e39-a9ef-600d04e88516,cd591d65-1e65-4962-a10a-0c3be73f2f06,547696e9-6c86-4a45-9ca2-7eacc7e43627,2ac70519-d00d-4b50-aaee-13c8e7ac91ae,15794a01-13de-47a3-a7f2-d7f18c323971,d131034d-8f97-4044-8daf-13a70f875a2e,fbe6e24a-e7b2-4aec-aee0-a0d5ed4038b4,bff25a9f-c71f-4894-a083-18edd3799ad5,e9a15e59-478d-447d-8223-6b4c6caae87e,16400dcf-9a68-44e6-acbd-f345b9b47dba,4652316c-19f7-453d-ad69-d9a4d1acf6d9,9c5ab5e9-8b9d-463c-8c8f-97629762e151,18e27a53-9eb0-4043-b9c3-42872e2da3b0,bdd05421-3374-423e-8ff2-e2714d2f8755,ca5a4797-5206-412c-a8c6-4822c46dbd24,c631a824-92aa-4295-ad32-4fef49ec2399,a32fb232-1726-434c-81cb-168719d56615,30116527-1e5e-432e-a167-c13347aa683f,c7e66ce4-4031-44e3-8b37-cfb55e016f93,ebb1a513-5d25-4e69-8bee-f0cecb83614e,2f2fb0ca-935a-41f9-a3cd-6409eef05cb5,1e30330e-76a5-45d1-8cc6-5bccf47b63c0,8cb9b44c-3090-433a-be75-3c688eb4a084,568b0183-de7a-4080-9dd8-129b851e2671,da992fe6-02cd-4528-a08b-19cca4894d83,a298de35-de25-40ce-a207-3e9a21dd5563,331245dd-06c7-495b-8c4c-521ae93ace17,a20e5008-72aa-4d41-a269-3d8b89a1435f,75e17172-dc04-4159-ad9a-d9c0a11232a5,9148ca27-3896-425c-9c64-5f39c544a6c2,60f47ca2-c63d-4a64-af6f-94269474388f,05b13108-0772-4509-868c-5f862cd95ab4,e5f15ca3-2ee1-48dc-abf6-01edcd0c1a69,a16a23df-c715-4665-8903-2329c73290ac,95e340a1-7608-4f39-a912-02311a3bcca4,0498023c-8bd8-4e38-a82d-45ab989e0fb7,2381c2ab-f360-4776-ba39-f76cc9304162,2539e734-13c8-4d34-992f-f3b61bd0ed4a}	IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 1\nDeepFilter: An Instrumental Baseline for Accurate\nand Efficient Process Monitoring\nHao Wang, Zhichao Chenâ€ , Licheng Pan, Xiaoyu Jiang, Yichen Song, Qunshan He,\nXinggao Liuâ€ Member, IEEE\nAbstract â€”Effective process monitoring is increasingly vital in\nindustrial automation for ensuring operational safety, necessitat-\ning both high accuracy and efficiency. Although Transformers\nhave demonstrated success in various fields, their canonical form\nbased on the self-attention mechanism is inadequate for process\nmonitoring due to two primary limitations: (1) the step-wise\ncorrelations captured by self-attention mechanism are difficult\nto capture discriminative patterns in monitoring logs due to the\nlacking semantics of each step, thus compromising accuracy; (2)\nthe quadratic computational complexity of self-attention hampers\nefficiency. To address these issues, we propose DeepFilter, a\nTransformer-style framework for process monitoring. The core\ninnovation is an efficient filtering layer that excel capturing long-\nterm and periodic patterns with reduced complexity. Equipping\nwith the global filtering layer, DeepFilter enhances both accuracy\nand efficiency, meeting the stringent demands of process mon-\nitoring. Experimental results on real-world process monitoring\ndatasets validate DeepFilterâ€™s superiority in terms of accuracy\nand efficiency compared to existing state-of-the-art models.1\nIndex Terms â€”Energy security, Industrial time series analytics,\nProcess monitoring.\nI. I NTRODUCTION\nMONITORING quality variables through advanced time-\nseries analysis is paramount for ensuring operational\nsafety in industrial automation across a broad range of ap-\nplications [1â€“3]. These techniques are widely employed in\nprocess engineering [4], manufacturing [5, 6], and energy\nconversion [7], where the growing demands for higher effi-\nciency and cost-effectiveness often drive equipment to operate\nunder extreme conditions, thereby increasing the likelihood\nof catastrophic failures [7, 8]. For example, in chemical\nengineering, the Haber-Bosch process for ammonia synthesis\nrequires temperatures above 400Â°C and pressures exceeding\n200 bar, escalating risks of equipment failure and hazardous\nleaks [9, 10]. Similarly, nuclear power plants operate reactors\nat high pressures and temperatures to maximize output, height-\nening the threat of catastrophic radiation leakage [11, 12].\nThis work is supported by the National Natural Science Foundation of\nChina (11975207, 12075212, 12105246, 62073288), the National Key Re-\nsearch and Development Program of China (Grant No. 2021YFC2101100),\nand Zhejiang University NGICS Platform.\nHao Wang, Zhichao Chen, Licheng Pan, Xiaoyu Jiang, Yichen Song,\nQunshan He, and Xinggao Liu are with the State Key Laboratory\nof Industrial Control Technology, College of Control Science and En-\ngineering, Zhejiang University, Hangzhou 310027, China (e-mail: hao-\nhaow@zju.edu.cn; 12032042@zju.edu.cn; 22132045@zju.edu.cn; jiangxi-\naoyu@zju.edu.cn syc 1203@zju.edu.cn; heqs@zju.edu.cn; lxg@zju.edu.cn).\n1Code is available in an anonymous repository: https://anonymous.4open.\nscience/r/DeepFilter-BC3B. The dose rate data is daily updated on our project\nwebsite, and we commit to releasing them after peer review completes.These examples underscore the pressing need for advanced\nprocess monitoring systems to mitigate risks, reduce costs,\nand ensure safety, reliability, and efficiency in industrial au-\ntomation [3, 13, 14].\nProcess monitoring aims to estimate next-step values of\nquality variables from historical logs, identifying anomalies\nwhen estimated values deviate significantly from observations.\nUnlike other industrial data analytics tasks, such as fault\ndiagnosis [15, 16] or soft sensing [17â€“19], process monitoring\nrequires not only high accuracy but also operational effi-\nciency for real-time decision-making. In large-scale chemical\nplants [9, 10], for instance, precise estimates help detect\nsubtle shifts in temperature or pressure, enabling operators to\nintervene before potential reactor instabilities escalate. Equally\nimportant, real-time efficiency ensures these corrective mea-\nsures are executed promptly, minimizing the risk of safety\nincidents or costly downtime. Thus, an instrumental process\nmonitoring system should predict the next-step values of\nquality variables both accurately and efficiently , fulfilling its\npivotal role in safeguarding industrial equipment.\nTo achieve accurate and efficient process monitoring, a va-\nriety of data-driven algorithms have been developed, evolving\nfrom early identification methods to advanced deep learning\nmodels. Traditional methods such as ARIMA [20] offer com-\nputational simplicity but are limited in handling the nonlinear\ncomplexity of industrial data. Statistical methods, includ-\ning decision trees [21], XGBoost [22], and regression-based\napproaches [23], provide improved accuracy through richer\nfeature extraction but rely heavily on manual engineering and\nstruggle to scale with large datasets. In contrast, deep learning\narchitecturesâ€”spanning convolutional [24], recurrent [25, 26],\nand graph neural networks [27]â€”enable automated feature\nextraction and GPU acceleration, driving both performance\nand speed. Notably, Transformers [28] have emerged as an\nexemplar solution, featured by the self-attention layer for\ncapturing temporal patterns. This layer dynamically computes\nweighted dependencies across all time steps in the monitor-\ning logs, capturing step-wise relationships useful for predic-\ntion [29]. Additionally, the architecture is optimized for GPU\nacceleration, well-suited for large-scale monitoring data [28]\nprocessing. These advancements have made Transformers a\npreferred choice in industrial time-series analytics [29â€“31].\nDespite the promise of Transformers, we contend that\ncanonical self-attention layers present two critical shortcom-\nings that hinder their suitability for the stringent demands\nof process monitoring. First, step-wise correlations in self-\nattention fail to adequately represent the discriminative, long-arXiv:2501.01342v1  [cs.AI]  2 Jan 2025IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 2\nterm patternsâ€”such as trends and periodicitiesâ€”crucial for\ndetecting anomalies in process data. Individual observations\nin industrial logs often lack semantic richness, hindering the\nself-attention layer from capturing the discriminative patterns\nfor accurate monitoring. Second, the quadratic computational\ncomplexity of self-attention poses serious challenge, especially\nin real-time monitoring applications where low latency is\nparamount. Consequently, canonical Transformers fall short\nof delivering the required accuracy and efficiency in process\nmonitoring.\nTo overcome these limitations, we propose DeepFilter , a\nrefined Transformer architecture that replaces the self-attention\nlayer with a novel global filtering block specifically tailored\nfor process monitoring. This block performs adaptive filtering\nacross the entire temporal sequence, effectively modeling the\nlong-term discriminative patterns. Theoretical analysis further\nconfirms its capacity to enhance representations of long-term\ndiscriminative patterns. Moreover, by discarding the quadratic\ncomplexity inherent in self-attention, the global filtering block\nsignificantly reduces computational overhead. Extensive evalu-\nations on real-world datasets demonstrate that DeepFilter con-\nsistently delivers superior accuracy and efficiency relative to\nstate-of-the-art models, highlighting its role as an instrumental\nbaseline for Transformer-based process monitoring.\nOrganization. Section II provides a detailed description of\nthe DeepFilter architecture. Section III presents a case study\non the process monitoring for real-world nuclear power plants,\ndemonstrating the improvements in accuracy and efficiency\nachieved by DeepFilter. Section IV offers a review of related\nworks in process monitoring and highlight the contribution\nof this work in the context of existing studies. Finally, we\nsummarize our conclusions, limitations and outline directions\nfor future research.\nII. M ETHODOLOGY\nIn response to the inherent limitations of Transformer in ac-\ncurate and efficient process monitoring, this section introduces\nthe DeepFilter approach. DeepFilter replaces the self-attention\nlayer in Transformer with an efficient filtering layer for fusing\nthe information across different steps, effectively enhancing\naccuracy and operational efficiency for process monitoring.\nA. Problem Definition\nA monitoring log consists of a chronological sequence\nof observations [L(1), L(2), . . . , L (P)], where each L(t)âˆˆ\nR1Ã—Dinrepresents the observation at the t-th step with Din\ncovariates. We define XâˆˆRTÃ—Dinas the historical sequence\nandyHâˆˆRas the quality variable, where T is the length of\nthe historical window and H is the monitoring horizon. At an\narbitrary time step t, the historical sequence is represented as\nX= [L(tâˆ’T + 1) , . . . , L (t)], and the corresponding quality\nvariable is specified as the final feature in L(t+ H) .\nThe objective of process monitoring is to develop a pre-\ndictive model g:RTÃ—Dinâ†’Rthat generates the quality\nvariable perdition g(X) = Ë† yHâ†’yH. In practical process\nmonitoring applications, the training dataset predominantly\ncomprises normal operational logs. Consequently, anomalies\nÃ—KEfficient filtering layerLayer-normFFN layerLayer-norm\nAffinelayerGRUlayer\nFFT\nIFFT\nğ™âˆˆâ„!Ã—#ğ™(%)âˆˆâ„‚!Ã—#ğ‘¾(%)ğ™&âˆˆâ„!Ã—#\nğ™âˆˆâ„!Ã—#ğ‘âˆˆâ„!Ã—#ğ‘¦)'âˆˆâ„\nğ—âˆˆâ„!Ã—#GF block\nTemporal tensor\nFrequency tensorFig. 1. Overview of the core components in DeepFilter.\nare detected as significant deviations between the actual and\npredicted quality variable values.\nB. Global Filtering Block\nThe fundamental component of DeepFilter is the Global Fil-\ntering (GF) block, as illustrated in Fig. 1, which integrates an\nefficient filtering layer for mixing information across different\ntime steps and a feed-forward network (FFN) layer for mixing\ninformation across different channels.\nLetZâˆˆRTÃ—Ddenote the input sequence to the k-th GF\nblock, where Tis the window length of historical monitoring\nlogs and Dis the hidden dimension. The global filtering\nprocess begins by transforming Zfrom the time domain to the\nfrequency domain using the Fast Fourier Transform (FFT):\nZ(F)=F(Z), (1)\nwhere Fdenotes the FFT operation. In the frequency domain,\nnoisy and discriminative patterns are often easily isolated.\nTypically, noisy patterns often reside in high-frequency com-\nponents [32â€“34], while discriminative patterns often emerge\nin low-frequency components [35, 36]. To extract the discrim-\ninative patterns and suppress the noisy ones, we perform a\nfiltering operation using the Hadamard product:\nÂ¯Z(F)=Z(F)âŠ™Â¯W, (2)\nwhere Â¯WâˆˆCTÃ—Dcontains learnable parameters that are\noptimized to discern discriminative patterns in model training.\nThe filtered sequence is then transformed back to the time\ndomain via the inverse FFT:\nÂ¯Z=Fâˆ’1(Â¯Z(F)), (3)IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 3\nwhich is immediately followed by a residual connection and\nlayer normalization to stabilize the training process and miti-\ngate gradient degradation:\nR= LayerNorm( Â¯Z+Z), (4)\nwhich is the output of the efficient filtering layer. To demon-\nstrate the efficacy of the operations in this layer, we restate\nthe convolution theorem below.\nTheorem II.1. Suppose W=Fâˆ’1(W(F)), â€âˆ—â€ is the\ncircular convolution operator, the filtered sequence in (3)can\nbe acquired by performing circular convolution below\nÂ¯Z=Wâˆ—Z.\nProof. It is equivalent to prove F(Wâˆ—Z) =Z(F)âŠ™Â¯W. To\nthis end, the n-th element of the circular convolution above\ncan be expressed as follow\nÂ¯Zn=Tâˆ’1X\nm=0WmZ(nâˆ’m)%T\nOn the basis, the FFT of Â¯Zis denoted as Â¯Z(F), where the Ï‰-th\nelement is given by:\nÂ¯Z(F)\nÏ‰=Tâˆ’1X\nn=0Tâˆ’1X\nm=0WmZ(nâˆ’m)%Teâˆ’2Ï€i\nTnÏ‰\n=Tâˆ’1X\nn=0Tâˆ’1X\nm=0Wmeâˆ’2Ï€i\nTmÏ‰Z(nâˆ’m)%Teâˆ’2Ï€i\nTÏ‰(nâˆ’m)\n=Tâˆ’1X\nm=0Wmeâˆ’2Ï€i\nTmÏ‰Tâˆ’1X\nn=0Z(nâˆ’m)%Teâˆ’2Ï€i\nTÏ‰(nâˆ’m)\n=W(F)\nÏ‰Tâˆ’1X\nn=0Z(nâˆ’m)%Teâˆ’2Ï€i\nTÏ‰(nâˆ’m)\n=W(F)\nÏ‰Tâˆ’1X\nn=mZnâˆ’meâˆ’2Ï€i\nTÏ‰(nâˆ’m)\n+W(F)\nÏ‰mâˆ’1X\nn=0Znâˆ’m+Teâˆ’2Ï€i\nTÏ‰(nâˆ’m)\n=W(F)\nÏ‰(Tâˆ’mâˆ’1X\nn=0Zneâˆ’2Ï€i\nTÏ‰(n)+Tâˆ’1X\nn=Tâˆ’mZneâˆ’2Ï€i\nTÏ‰(n))\n=W(F)\nÏ‰Â·Z(F)\nÏ‰.\nThus, the equation F(Wâˆ—Z) =Z(F)âŠ™Â¯Wholds, and the\nproof is thereby completed.\nTheoretical implications. Theorem II.1 implies that the effi-\ncient filtering layer adepts at accuracy and efficiency, meeting\nthe dual excessive demand of process monitoring.\nâ€¢The efficient filtering layer excel capturing discriminant tem-\nporal patterns in the historical sequence, thereby improving\naccuracy. According to Theorem II.1, the layer is equivalent\nto a circular convolution between the historical sequence and\na large convolution kernel, where the kernel size equals the\nhistorical window length T. Circular convolution facilitates\nthe capturing of periodic patterns, while the large kernel sizefacilitates the modeling of long-term dependencies. Both pe-\nriodic and long-term patterns are typically discriminative for\nprocess monitoring, in contrast to the step-wise correlations\ncaptured by standard Transformers.\nâ€¢The efficient filtering layer reduces the computational com-\nplexity, thereby improving efficiency. The overall complex-\nity of the efficient filtering layer is O(T log T) , significantly\nlower than that of self-attention layers and convolution\nlayers with a full receptive field ( O(T2)).\nWhile the filtering layer captures dominant temporal pat-\nterns in each channel, it does not incorporate channel-wise\ninteractions. To fill in the gap, we introduce FFN as follows:\nFFN( R) = ReLU( RW(1)+b(1))W(2)+b(2),(5)\nÂ¯R= LayerNorm(FFN( R) +R), (6)\nwhere W(1),b(1),W(2)andb(2)are learnable parameters.\nTo stabilize the training process, residual connection and layer\nnormalization are subsequently applied.\nIn a nutshell, the GF block captures temporal and channel-\nwise patterns via the efficient filtering layer and the FFN layer,\nrespectively, contributing to a representation Â¯RâˆˆRTÃ—Dthat\ncomprehensively understands the process monitoring logs.\nC. DeepFilter Architecture and Learning Objective\nThe GF block efficiently processes historical monitoring\nlogs and excels at capturing discriminative temporal patterns.\nHowever, this block focuses on encapsulating these patterns\ninto a compact representation Â¯R, without generating the pre-\ndicted value of quality variable for process monitoring. To\nbridge this gap, we introduce DeepFilter, which integrates\ncascaded GF blocks for achieving process monitoring.\nThe architecture of DeepFilter is illustrated in Fig. 1. It\nbegins by transforming the historical monitoring log Xâˆˆ\nRTÃ—Dininto a latent representation through an affine layer:\nZ0=XW(0)+b(0)(7)\nwhere Z0âˆˆRNÃ—Drepresents the initial embeddings with\nhidden dimension D,W(0),b(0)are learnable parameters.\nThese embeddings are then sequentially processed through K\nGF blocks. Let Zkdenote the input to the k-th GF block, the\noutput of this block is given by:\nÂ¯Rk:= GF k(Zk), (8)\nwhere GFk(Â·)performs the transformations from Eq (2) to\n(6) sequentially. The output of each GF block serves as\nthe input for the subsequent block, i.e., Zk+1:=Â¯Rk. The\noutput from the last GF block, denoted as Â¯RKâˆˆRTÃ—D,\nencapsulates the historical monitoring logs comprehensively.\nThis representation is passed to a Gated Recurrent Unit (GRU)\ndecoder, and the final-step output of the GRU decoder serves\nas the prediction of the quality variable:\nË†yH:= GRU( Â¯RK), (9)\nwhere Ë†yHis the estimated value of the target quality variable.\nThere are several learnable parameters in DeepFilter, such\nas the weights and biases in the FFN, the affine layer, theIEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 4\n(a) National station distribution.\nA: Radiation monitoring station\nB: Weather sensors\nC: Spectrometer\nD: Ionization chamber\nE: Aerosol sampler\nF: Rain gauge\nG: Settlement samplerC\nAB\nF\nGED (b) Composition of a single station.\nFig. 2. Overview of the nuclear monitoring stations in our project.\nFig. 3. Scene photo of monitoring stations and key sensors.\nGRU decoder, and the filter tensor W(F). These parameters\nare optimized by minimizing the mean squared error (MSE)\nbetween the predicted and actual quality variable values,\ndefined as:\nL:=\nyHâˆ’Ë†yH2. (10)\nIII. E XPERIMENTS\nThis section aims to empirically validating the effectiveness\nof DeepFilter in the context of process monitoring. To this end,\nthere are three aspects need to be investigated.\n1)Accuracy: Does DeepFilter work effectively? Section III-C\ncompares the accuracy of DeepFilter against baselines on\ntwo large-scale real-world process monitoring datasets.\n2)Efficiency: Does DeepFilter work efficiently? Section III-D\nevaluates the actual running time of DeepFilter and Trans-\nformers under varying configurations.\n3)Sensitivity: Is DeepFilter sensitive to hyperparameter vari-\nation? Section III-E evaluates and analyzes the perfor-\nmance of DeepFilter under varying hyperparameter values.\nA. Background and Data Collection\nNuclear power plants (NPPs) play a crucial role in in-\ndustrial automation, providing a stable and efficient energy\nsource. In the United States, NPPs produce nearly 800 billion\nkilowatt-hours of electricity annually, accounting for over 60%\nof the nationâ€™s emission-free electricity [37]. This reduces\napproximately 500 million metric tons of carbon emissions,\ndemonstrating their environmental and industrial significance.\nHowever, NPPs also pose security risks, as operational anoma-\nlies can lead to radionuclide leaks, resulting in severe en-\nvironmental pollution and casualties [11]. To mitigate these\nrisks, automated monitoring networks have been deployedTABLE I\nVARIABLES IN THE MONITORING LOG\nField Unit of measure\nAtmospheric radiation\nDose rate nGy / h\nSpectrometer measures (1024 channels) ANSI/IEEE N42.42\nMeteorological conditions\nTemperatureâ—¦C\nHumidity %\nAtmosphere pressure hPa\nWind direction Clockwise angle\nWind speed m / s\nPrecipitation indicator Boolean value\nAmount of precipitation mm\nSpectrometer operating conditions\nBattery voltage V\nSpectrometer voltage V\nSpectrometer temperatureâ—¦C\nTABLE II\nDESCRIPTION OF SAMPLING STRATEGY .\nStation Location #Sample #Variable Interval\nXingan Station Hegang 38,686 1,034 5 min\nHuayuan Station Jinan 38,687 1,034 5 min\nworldwide, such as the RadNet in the United States [38], the\nFixed Point Surveillance Network in Canada [39], and the At-\nmospheric Nuclear Radiation Monitoring Network (ANRMN)\nin China, as shown in Fig. 2. These systems enable continuous,\nreliable monitoring of radionuclide concentrations, reflecting\nwhether NPPs are operating normally.\nThe key quality variable monitored by these systems is\nthe atmospheric Î³-ray dose rate, measured using ionization\nchambers (Fig. 3) [40]. Fig. 4(a) illustrates the non-stationary\ndynamics of this variable, likely influenced by external fac-\ntors such as weather conditions. To account for these in-\nfluences, the monitoring systems include additional process\nvariables, such as spectrometer measurements (spanning 1024\nchannels), meteorological conditions (e.g., precipitation), and\nspectrometer operational parameters (e.g., battery voltage).\nThese process variables, shown in Fig. 4(b-d), display diverse\ntemporal patterns. Frequency domain analysis reveals concen-\ntrated energy in low-frequency bands, which diminishes at\nhigher frequencies, highlighting the potential of frequency-\nbased models to extract semantic-rich representations.\nThe monitoring logs integrate 1,024-channel spectrometer\ndata, seven meteorological covariates, and three operational\nparameters, as summarized in Table I. By consolidating diverse\ndata sources, these logs provide a comprehensive view of\nNPP operational status, facilitating the construction of process\nmonitoring systems to safeguard its operations.\nB. Experimental Setup\n1) Datasets: We employ two industrial datasets sourced\nfrom monitoring logs collected from the ANRMN project.\nThese datasets encompass 1034 input variables, as detailed\nin Table I. The statistics of preprocessed data is present in\nTable II. Each dataset is sequentially divided into training,IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 5\n0 2000 4000 6000 8000 10000\nTime stamp0.00.51.0Volumn\n(a) The dynamics of the quality variable: dose rate.\n0 2000 4000 6000 8000 10000\nTime stamp0.00.51.0Volumn\n0 1000 2000 3000 4000 5000\nFrequency bin010002000Volumn\n(b) The 64-th channel of the spectrometer measurements and its FFT.\n0 2000 4000 6000 8000 10000\nTime stamp0.00.51.0Volumn\n0 1000 2000 3000 4000 5000\nFrequency bin020004000Volumn\n(c) The temperature measurements and its FFT.\n0 2000 4000 6000 8000 10000\nTime stamp0.00.51.0Volumn\n0 1000 2000 3000 4000 5000\nFrequency bin020004000Volumn\n(d) The humidity measurements and its FFT.\nFig. 4. Dynamics of the quality variable (a) and some important process\nvariables (b-d) and in the collected dataset from Jinan station. We also provide\nthe amplitude characteristics of their FFT in (b-d).\nvalidation, and testing subsets with allocation ratios of 70%,\n15%, and 15%, respectively. To ensure effective model training\nand evaluation, the datasets are normalized using a min-max\nscaling technique.\n2) Baselines: We compare DeepFilter with three categories\nof baselines as follows:\nâ€¢Identification methods: AR, MA and ARIMA [20];\nâ€¢Statistical methods: Lasso Regression (LASSO) [23],\nSupport Vector Regression (SVR) [41], Random Forest\n(RF) [21] , and eXtreme Gradient Boosting (XGB) [22];\nâ€¢Deep methods: Long Short-Term Memory (LSTM) [26],\nGated Recurrent Unit (GRU) [25], Transformer [28],\nInformer [42], AttentionMixer [7] and iTransformer [43].\nAligning with the prevailing work [7], we employ a\nGRU decoder for transformer-based baselines to produce\nquality variable prediction.\n3) Training Strategy: All experimental procedures are ex-\necuted using the PyTorch framework, utilizing the Adam\noptimizer [44] for its adaptive learning rate capabilities and ef-\nficient convergence properties. The experiments are conducted\non a hardware platform comprising two Intel(R) Xeon(R)\nPlatinum 8383C CPUs operating at 2.70 GHz and eight\nNVIDIA GeForce 1080Ti GPU. Hyperparameter optimizationis systematically performed following the standard protocol [7]\nto enhance model performance. The learning rate is tuned\nwithin {0.001, 0.005, 0.01 }; the batch size is tuned within {32,\n64}; the number of GF blocks is set to 2; the historical window\nlength is set to 16. The model is trained for a maximum of\n200 epochs. An early stopping strategy with a patience of 15\nepochs is employed, stopping training if no improvement was\nobserved within the validation set. Finally, the performance\nmetrics on the test set is calculated and reported.\n4) Evaluation Strategy: The coefficient of determination\n(R2) is selected for evaluating monitoring accuracy:\nR2= 1âˆ’PN\nt=1\nyH\ntâˆ’Ë†yH\nt2\nPN\nt=1\nyH\ntâˆ’Â¯yH\nt2, (11)\nwhere yH\ntrepresents the actual dose rate, Ë†yH\ntdenotes the\nestimated dose rate by the model, and Â¯yH\ntis the mean value\nof the actual dose rates over the test set of size N. This\nmetric effectively captures the proportion of variance in yH\nthat is predictable from Ë†yH. We also incorporate the root mean\nsquared error (RMSE) and the mean absolute error (MAE) as\nsupplementary metrics, quantifying the average magnitude of\nthe prediction errors.\nRMSE =vuut1\nNNX\nt=1(yH\ntâˆ’Ë†yH\nt)2,\nMAE =1\nNNX\nt=1|(yH\ntâˆ’Ë†yH\nt)|.(12)\nC. Overall Performance\nIn this section, we compare the monitoring accuracy of\nDeepFilter and baselines across four distinct forecast horizons\n(H = 1 ,3,5,7). Results are shown in Table III with key\nobservations below:\nâ€¢Identification models demonstrate limited accuracy in pro-\ncess monitoring. Primarily designed to capture linear au-\ntocorrelations, identification models struggle to model the\nnon-linear patterns that are prevalent in many datasets. This\nlimitation becomes more pronounced in long-term forecast-\ning scenarios, where ARIMA models, for instance, record\nrelatively high MAE of 0.158 and 0.127 on the Hegang and\nJinan datasets, respectively, for H = 7 .\nâ€¢Statistic models integrate external factors, such as me-\nteorological conditions, and demonstrate competitive per-\nformance in short-term monitoring. Non-linear estimators,\nparticularly XGBoost, outperform linear models due to\ntheir higher modeling capacities. Notably, XGBoost exhibits\nrobust performance across both datasets, achieving results\ncomparable to traditional deep learning models like LSTM\nand GRU across most evaluation metrics.\nâ€¢Deep models achieve the best performance among baselines.\nSpecifically, Transformer-based methods display varying\naccuracy contingent upon their temporal fusing mechanisms.\nAmong them, the standard Transformer model, utilizing\nself-attention mechanisms, shows suboptimal monitoring\naccuracy, highlighting the limitations of self-attention inIEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 6\nTABLE III\nCOMPARATIVE STUDY ON THE HEGANG AND JINAN DATASETS OVER FOUR FORECAST HORIZONS .\nMethodsH=1 H=3 H=5 H=7\nMAE R2 MAE R2 MAE R2 MAE R2\nDataset: Hegang Station\nAR 0.135 Â±0.000 0.064Â±0.000 0.137Â±0.000 -0.038Â±0.000 0.140Â±0.000 -0.111Â±0.000 0.141Â±0.000 -0.163Â±0.000\nMA 0.133 Â±0.000 0.110Â±0.000 0.136Â±0.000 -0.042Â±0.000 0.138Â±0.000 -0.096Â±0.000 0.140Â±0.000 -0.158Â±0.000\nARIMA 0.135 Â±0.000 0.063Â±0.000 0.148Â±0.000 -0.246Â±0.000 0.147Â±0.000 -0.238Â±0.000 0.143Â±0.000 -0.256Â±0.000\nLASSO 0.045 Â±0.000 0.282Â±0.000 0.046Â±0.000 0.243Â±0.000 0.047Â±0.000 0.183Â±0.000 0.049Â±0.000 0.109Â±0.000\nSVR 0.023 Â±0.000 0.890Â±0.000 0.034Â±0.000 0.779Â±0.000 0.051Â±0.000 0.540Â±0.000 0.058Â±0.000 0.391Â±0.000\nXGBoost 0.016 Â±0.000 0.943Â±0.000 0.018Â±0.000 0.902Â±0.000 0.022Â±0.000 0.838Â±0.000 0.023Â±0.000 0.778Â±0.000\nLSTM 0.021 Â±0.001 0.846Â±0.011 0.023Â±0.001 0.790Â±0.013 0.026Â±0.001 0.718Â±0.015 0.027Â±0.002 0.651Â±0.024\nGRU 0.023 Â±0.001 0.832Â±0.008 0.025Â±0.001 0.773Â±0.011 0.028Â±0.002 0.693Â±0.024 0.030Â±0.002 0.631Â±0.019\nTransformer 0.015 Â±0.002 0.927Â±0.022 0.020Â±0.001 0.845Â±0.009 0.027Â±0.001 0.675Â±0.004 0.031Â±0.002 0.557Â±0.015\nInformer 0.042 Â±0.017 0.562Â±0.269 0.035Â±0.004 0.575Â±0.114 0.037Â±0.008 0.498Â±0.102 0.036Â±0.002 0.483Â±0.014\niTransformer 0.014 Â±0.001 0.919Â±0.026 0.016Â±0.003 0.911Â±0.025 0.018Â±0.002 0.853Â±0.019 0.032Â±0.012 0.660Â±0.134\nAttentionMixer 0.011Â±0.003 0.962Â±0.012 0.017Â±0.005 0.901Â±0.041 0.018Â±0.003 0.832Â±0.043 0.023Â±0.003 0.766Â±0.040\nDeepFilter 0.012 Â±0.001 0.963Â±0.006 0.015Â±0.001 0.927Â±0.004 0.018Â±0.001 0.860Â±0.018 0.022Â±0.002 0.763Â±0.053\nDataset: Jinan Station\nAR 0.106 Â±0.000 0.744Â±0.000 0.118Â±0.000 0.668Â±0.000 0.123Â±0.000 0.620Â±0.000 0.130Â±0.000 0.571Â±0.000\nMA 0.112 Â±0.000 0.710Â±0.000 0.119Â±0.000 0.666Â±0.000 0.123Â±0.000 0.623Â±0.000 0.129Â±0.000 0.571Â±0.000\nARIMA 0.101 Â±0.000 0.759Â±0.000 0.111Â±0.000 0.710Â±0.000 0.118Â±0.000 0.646Â±0.000 0.127Â±0.000 0.607Â±0.000\nLASSO 0.029 Â±0.000 0.742Â±0.000 0.030Â±0.000 0.717Â±0.000 0.031Â±0.000 0.680Â±0.000 0.033Â±0.000 0.636Â±0.000\nSVR 0.061 Â±0.000 0.640Â±0.000 0.065Â±0.000 0.599Â±0.000 0.056Â±0.000 0.679Â±0.000 0.050Â±0.000 0.706Â±0.000\nXGBoost 0.031 Â±0.000 0.903Â±0.000 0.033Â±0.000 0.883Â±0.000 0.036Â±0.000 0.853Â±0.000 0.038Â±0.000 0.816Â±0.000\nLSTM 0.015 Â±0.000 0.948Â±0.001 0.017Â±0.000 0.921Â±0.005 0.019Â±0.000 0.888Â±0.004 0.021Â±0.000 0.853Â±0.004\nGRU 0.016 Â±0.001 0.939Â±0.005 0.018Â±0.000 0.914Â±0.006 0.020Â±0.000 0.882Â±0.005 0.022Â±0.001 0.842Â±0.012\nTransformer 0.016 Â±0.004 0.955Â±0.023 0.017Â±0.002 0.929Â±0.014 0.024Â±0.005 0.837Â±0.057 0.025Â±0.002 0.813Â±0.013\nInformer 0.018 Â±0.002 0.917Â±0.013 0.025Â±0.008 0.851Â±0.041 0.022Â±0.000 0.813Â±0.003 0.026Â±0.002 0.761Â±0.021\niTransformer 0.012 Â±0.000 0.971Â±0.005 0.014Â±0.002 0.948Â±0.028 0.018Â±0.002 0.906Â±0.023 0.020Â±0.004 0.881Â±0.033\nAttentionMixer 0.011 Â±0.004 0.981Â±0.012 0.012Â±0.002 0.967Â±0.007 0.016Â±0.002 0.912Â±0.035 0.027Â±0.010 0.840Â±0.039\nDeepFilter 0.010Â±0.002 0.986Â±0.003 0.012Â±0.000 0.970Â±0.004 0.015Â±0.001 0.940Â±0.007 0.019Â±0.003 0.890Â±0.023\nNote : The results are reported in meanÂ±std. The best and second best metrics are bolded and underlined, respectively.\ncapturing discriminative patterns within this context. Modi-\nfications to the token mixer, such as iTransformer and Atten-\ntionMixer, result in significant performance enhancements.\nThis suggests that the refinement of token mixer is critical\nfor accommodating Transformers to process monitoring.\nâ€¢DeepFilter demonstrates the best overall performance across\ndifferent metrics and datasets. Its superior accuracy demon-\nstrates that the efficient filtering layer excels at captur-\ning long-term discriminant patterns, as discussed in The-\norem II.1, which facilitates understanding ECP logs.\nIn-depth analysis. We conduct a detailed comparison of the\nmonitoring performance of DeepFilter and the Transformer\nmodel with self-attention for temporal fusion, focusing on\nthree key aspects in Fig. 5:\nâ€¢Predicted series. The left panels illustrate the predicted\ntime-series values against the ground truth. DeepFilter con-\nsistently tracks the ground truth more accurately, especially\nduring regions with sharp fluctuations and peaks. For in-\nstance, in both datasets, the Transformer fails to capture\nsudden spikes around timestamps 1700 and 1800. DeepFil-\nter, in contrast, remains utility in these cases, effectively\nfollowing both gradual trends and abrupt changes.\nâ€¢Error distribution. The middle panels display the distribu-\ntion of MAE. Overall, the Transformerâ€™s error distribution\nis more dispersed, with a heavier tail extending to higher\nerror values, indicating occasional significant deviations. In\ncontrast, DeepFilter exhibits a more concentrated error dis-\ntribution with consistently lower prediction errors, making\nit more reliable for critical monitoring tasks.TABLE IV\nCOMPLEXITY ANALYSIS\nBlocks Complexity Sequential Ops Path Length\nRNN O(TÂ·D2) O(T) O(T)\nTransformer O(T2Â·D) O(1) O(1)\niTransformer O(TÂ·D2) O(1) O(1)\nDeepFilter O(TÂ·log TÂ·D) O(1) O(1)\nâ€¢Fitting goodness. The right panels display scatter plots of\npredicted values against the ground truth. DeepFilterâ€™s pre-\ndictions form a tighter cluster along the diagonal, indicating\nmore accurate predictions overall. In contrast, the Trans-\nformer shows greater dispersion, particularly in higher value\nranges, with more extreme outliers. Therefore, DeepFilter\nis advantageous in providing predictions that consistently\nmatch the ground truth and reducing large errors.\nD. Complexity Analysis\nIn this section, we analyze and evaluate the computational\ncost of DeepFilter and baseline models [28]. Models are\nemployed to transform a sequence (x1, ...,xT)into another\nsequence (z1, ...,zT)of equal length Tand feature number D.\nThree key metrics are considered: complexity, sequential op-\nerations, and path length, respectively quantifying the amount\nof floating-point operations, the number of non-parallelizable\noperations, and the minimum number of layers required to\nmodel relationships between any two time steps.IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 7\n1000 1200 1400 1600 1800 2000\nTime stamp0.10.20.30.40.5ValueGround truth Transformer DeepFilter\n0.00 0.05 0.10 0.15 0.20\nVolume of MAE0100200300CountDeepFilter Transformer\n0.1 0.2 0.3 0.4 0.5\nGround truth0.10.20.30.40.5Prediction\nGround truth Transformer DeepFilter\n(a) Performance comparison on the Hegang dataset.\n1000 1200 1400 1600 1800 2000\nTime stamp0.050.100.15ValueGround truth Transformer DeepFilter\n0.00 0.05 0.10 0.15 0.20\nVolume of MAE0100200300400CountDeepFilter Transformer\n0.025 0.050 0.075 0.100 0.125 0.150 0.175\nGround truth0.050.100.15Prediction\nGround truth Transformer DeepFilter\n(b) Performance comparison on the Jinan dataset.\nFig. 5. In-depth comparison on the monitoring performance of DeepFilter and Transformer.\n32 64 128 256 512 1024\nHistorical length (L)10âˆ’310âˆ’210âˆ’1Time (s)\niTransformer\nTransformer\nDeepFilter\n32 64 128 256 512 1024\nFeature Number (D)10âˆ’310âˆ’210âˆ’1Time (s)\niTransformer\nTransformer\nDeepFilter\n(a) Inference time on the Intel(R) Xeon(R) Gold 6140 CPU.\n32 64 128 256 512 1024\nHistorical length (L)10âˆ’310âˆ’2Time (s)\nDeepFilter\niTransformer\nTransformer\n32 64 128 256 512 1024\nFeature Number (D)10âˆ’310âˆ’2Time (s)\nDeepFilter\niTransformer\nTransformer\n(b) Inference time on the Nvidia RTX Titan GPU.\nFig. 6. Inference time given varying settings, with solid lines for mean values\nof 10 trials and shaded areas for 90% confidence intervals. The default values\nof L, D and batch size are 16, 32, and 64, respectively.\nâ€¢The theoretical results are presented in Table IV. Specif-\nically, RNNs exhibit inefficiency due to heavy sequential\nops and lengthy path lengths. In contrast, Transformer and\niTransformer reduce sequential operations and path lengths,\nenabling them to process entire sequences within a single\ncomputational block and leverage GPU acceleration effec-\ntively. However, they incur quadratic complexity relative to\nT and D, respectively, producing inefficiency when dealing\nwith long sequences or numerous covariates.\nâ€¢DeepFilter addresses these limitations by incorporating a\nfiltering layer that reduces the computational complexity\ntoO(TÂ·log TÂ·D), which is notably lower than that of\nTransformer and iTransformer models, while maintainingthe minimum sequential ops and path length.\nâ€¢The empirical results are presented in Table III-D, compar-\ning the actual inference times of iTransformer, Transformer,\nand DeepFilter under various conditions. These results\ncorroborate the theoretical complexity in Table IV, with\nDeepFilter consistently outperforming both iTransformer\nand Transformer in terms of inference speed.\nThe results above underscore DeepFilterâ€™s superior effi-\nciency and scalability, making it highly suitable for practical\nprocess monitoring applications where the operational effi-\nciency of monitoring system is a demanding factor.\nE. Parameter Sensitivity Study\nIn this section, we investigate the impact of key hyperpa-\nrameters on DeepFilterâ€™s performance, including the number\nof global filtering blocks (K), window length (L), the number\nof hidden dimensions (D), and batch size. The results are\nsummarized in Fig. 7 with key observations as follows:\nâ€¢DeepFilterâ€™s performance is not significantly dependent on\na deep stack of blocks. As illustrated in Fig. 7(a), utilizing\n1-2 blocks already yields promising results. While adding\nmore blocks can incrementally improve performance, there\nis a risk of performance degradation, potentially due to\noverfitting and optimization challenges.\nâ€¢The modelâ€™s effectiveness improves with the inclusion of\nadequate historical monitoring data. In Fig. 7(b), we observe\nthat performance is suboptimal at L=4 but significantly\nenhances at L=8. Extending the window length further can\nlead to improved monitoring accuracy, indicating the value\nof incorporating sufficient historical context in the model.\nâ€¢The relationship between the number of hidden dimensions\nand performance does not follow a clear pattern as per\nFig. 7(c). A smaller dimension appears sufficient to effec-\ntively model the monitoring log at both stations, suggesting\nthat the logs contain a high degree of redundancy. Finally,IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 8\n1 2 3 4\nK0.950.960.970.980.99R2\nHegang Jinan\n1 2 3 4\nK0.010.02RMSE\nHegang Jinan\n(a) Performance with varying numbers of blocks (K).\n48243264\nL0.920.950.98R2\nHegang Jinan\n48243264\nL0.010.02RMSE\nHegang Jinan\n(b) Performance with varying settings of window length (L).\n48243264\nD0.950.970.99R2\nHegang Jinan\n48243264\nD0.010.02RMSE\nHegang Jinan\n(c) Performance with varying numbers of hidden dimensions (D).\n8 16 32 128\nBatch size0.950.970.99R2\nHegang Jinan\n8 16 32 128\nBatch size0.010.02RMSE\nHegang Jinan\n(d) Performance with varying settings of batch size.\nFig. 7. Parameter sensitivity study results on Hegang and Jinan datasets,\nwhere the dark areas indicate the 90% confidence intervals.\nenlarging batch sizes generally improves performance in\nFig. 7(d), which suggests the potential benefit of increasing\nthe batch size to further enhance model performance.\nIV. R ELATED WORKS\nData-driven process modeling has become a cornerstone\nof industrial automation with the rise of Industry 4.0 and\ndigital factory concepts [45â€“47]. These methods leverage large\nvolumes of monitoring data to enhance operational safety\nacross diverse industrial applications [4, 5, 48]. Current ap-\nproaches can be categorized into three groups: identification\nmethods, statistical methods, and deep learning approaches,\neach offering distinct advantages and limitations.\nEarly identification methods, such as Auto-Regressive (AR),\nMoving Average (MA), and Auto-Regressive Integrated Mov-\ning Average (ARIMA)[20], provide computational simplicity\nand real-time processing capabilities. However, their inabilityto capture nonlinear temporal dependencies limits their ef-\nfectiveness. Statistical methods, including decision trees[21],\nXGBoost [22], and generalized linear models [23], were\nsubsequently introduced to address these limitations. These\nmethods improve accuracy by capturing nonlinear patterns,\nbut they rely heavily on manual feature engineering and face\nscalability issues in large-scale industrial settings.\nThe advent of deep learning has revolutionized data-driven\nprocess monitoring, enabling automatic feature extraction and\nenhanced parallel computing capabilities. Various architec-\ntures, such as Convolutional Neural Networks (CNNs) [24],\nRecurrent Neural Networks (RNNs) [25, 26], and Graph\nNeural Networks (GNNs) [27], have been developed to extract\ndiscriminative representations from monitoring logs for next-\nvalue prediction. For example, a spatiotemporal attention-\nbased RNN model [45] is proposed to capture the nonlinearity\namong process variables and their temporal dynamics; a multi-\nscale attention-enhanced CNN [31] is proposed to identify\nlong- and short-term patterns; a multi-scale residual CNN [49]\nis proposed to extract high-dimensional nonlinear features at\nmultiple scales. These examples highlight the versatility of\ndeep learning in the context of process monitoring.\nBuilding on the success of deep learning methods above,\nTransformers [28] have gained prominence in process moni-\ntoring due to their scalability and parallel computing capabili-\nties [29â€“31]. Early applications utilized self-attention to model\nstep-wise relationships within monitoring logs [31, 50, 51]\nSubsequent works mainly enhanced the attention mechanisms\nfor time-series, such as FedFormer [52] for noise filtering,\nInformer [42] for reduced redundancy, Pyraformer [53] for\nmulti-scale dependencies, and LogTrans [54] for locality\nenhanced representation. However, the step-wise correlation\ncaptured by self-attention struggles to capture discriminative\npatterns in industrial logs due to the lack of semantic richness\nin individual observations. Recognizing this issue, another\nline of works [7, 43] advocated applying self-attention to\nmodel variate-wise correlations, which is more semantically\nmeaningful than step-wise correlations in process monitoring.\nWhile advanced models offer better predictive accuracy, the\nincreased size and complexity hinder practical deployment in\nmonitoring scenarios with strict latency and computational\nconstraints. To address this issue, research has explored dis-\ntributed modeling [55, 56] and sparsification techniques [57â€“\n59], which are effective to enhanced efficiency but entails\nadditional hard-core resources or scarifies some accuracy.\nTherefore, there exists a critical need for novel solutions that\nco-optimize accuracy and efficiency in process monitoring.\nDeveloping a modern Transformer-like architecture, satisfying\nthe accuracy and efficiency demands in real-time process\nmonitoring, remains an open question.\nV. C ONCLUSION\nIn this paper, we introduced DeepFilter, an adaptation of\nthe Transformer architecture specifically optimized for process\nmonitoring. By replacing the canonical self-attention layer in\nTransformer with an efficient global filtering layer, DeepFilter\nexcels at capturing long-term and periodic patterns inherentIEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 9\nin monitoring logs while significantly reducing computational\ncomplexity. Our experimental results on real-world process\nmonitoring datasets demonstrate that DeepFilter outperforms\nexisting state-of-the-art models in both accuracy and effi-\nciency, effectively meeting the stringent demands of modern\nprocess monitoring.\nLimitations and Future Work. The current implementation\nof DeepFilter utilizes the FFT for domain transformation,\nwhich is well-suited for typical stationary monitoring data but\nmay not effectively accommodate monitoring logs with swiftly\nvarying patterns. Future research could investigate alterna-\ntive transformation techniques, such as wavelet transforms\nor adaptive filtering methods, which are better equipped to\nhandle non-stationary monitoring logs. Another future work\nis deploying the proposed model on our online monitoring\nplatform to support national-wide nuclear power plants.\nREFERENCES\n[1] H. Wang, X. Liu, Z. Liu, H. Li, Y . Liao, Y . Huang, and Z. Chen,\nâ€œLspt-d: Local similarity preserved transport for direct industrial data\nimputation,â€ IEEE Trans. Autom. Sci. Eng. , 2024.\n[2] J. Yu, L. Ye, L. Zhou, Z. Yang, F. Shen, and Z. Song, â€œDynamic process\nmonitoring based on variational bayesian canonical variate analysis,â€\nIEEE Trans. Syst., Man, Cybern., Syst. , vol. 52, no. 4, pp. 2412â€“2422,\n2021.\n[3] Z. Yang and Z. Ge, â€œOn paradigm of industrial big data analytics: From\nevolution to revolution,â€ IEEE Trans. Ind. Informat. , vol. 18, no. 12,\npp. 8373â€“8388, 2022.\n[4] Z. Chen, H. Wang, G. Chen, Y . Ma, L. Yao, Z. Ge, and Z. Song,\nâ€œAnalyzing and improving supervised nonlinear dynamical probabilistic\nlatent variable model for inferential sensors,â€ IEEE Trans. Ind. Informat. ,\npp. 1â€“12, 2024.\n[5] H. Wang, Z. Chen, Z. Liu, L. Pan, H. Xu, Y . Liao, H. Li, and X. Liu,\nâ€œSpot-i: Similarity preserved optimal transport for industrial iot data\nimputation,â€ IEEE Trans. Ind. Informat. , pp. 1â€“9, 2024.\n[6] Z. Chen, H. Wang, Z. Song, and Z. Ge, â€œImproving data-driven inferen-\ntial sensor modeling by industrial knowledge: A bayesian perspective,â€\nIEEE Trans. Syst., Man, Cybern., Syst. , vol. 20, no. 11, pp. 13296â€“\n13307, 2024.\n[7] H. Wang, Z. Wang, Y . Niu, Z. Liu, H. Li, Y . Liao, Y . Huang, and\nX. Liu, â€œAn accurate and interpretable framework for trustworthy process\nmonitoring,â€ IEEE Trans. Artif. Intell , vol. 5, no. 5, pp. 2241â€“2252,\n2023.\n[8] H. Xu, Z. Liu, H. Wang, C. Li, Y . Niu, W. Wang, and X. Liu, â€œDenoising\ndiffusion straightforward models for energy conversion monitoring data\nimputation,â€ IEEE Trans. Ind. Informat. , 2024.\n[9] Z. Yang, T. Hu, L. Yao, L. Ye, Y . Qiu, and S. Du, â€œStacked dual-guided\nautoencoder: A scalable deep latent variable model for semi-supervised\nindustrial soft sensing,â€ IEEE Trans. Instrum. Meas. , vol. 73, pp. 1â€“14,\n2024.\n[10] Z. Chen, Z. Song, and Z. Ge, â€œVariational inference over graph:\nKnowledge representation for deep process data analytics,â€ IEEE Trans.\nKnowl. Data Eng. , vol. 36, no. 6, pp. 2730â€“2744, 2024.\n[11] F.-C. Chen and M. R. Jahanshahi, â€œNb-cnn: Deep learning-based crack\ndetection using convolutional neural network and na Â¨Ä±ve bayes data\nfusion,â€ IEEE Trans. Ind. Electron. , vol. 65, no. 5, pp. 4392â€“4400, 2018.\n[12] M. Embrechts and S. Benedek, â€œHybrid identification of nuclear power\nplant transients with artificial neural networks,â€ IEEE Trans. Ind. Elec-\ntron., vol. 51, no. 3, pp. 686â€“693, 2004.\n[13] L. Yao, W. Shao, and Z. Ge, â€œHierarchical quality monitoring for large-\nscale industrial plants with big process data,â€ IEEE Trans. Neural Netw.\nLearn. Syst. , vol. 32, no. 8, pp. 3330â€“3341, 2019.\n[14] L. Yao and Z. Ge, â€œIndustrial big data modeling and monitoring frame-\nwork for plant-wide processes,â€ IEEE Trans. Ind. Informat. , vol. 17,\nno. 9, pp. 6399â€“6408, 2020.\n[15] X. Jiang and Z. Ge, â€œData augmentation classifier for imbalanced fault\nclassification,â€ IEEE Trans. Autom. Sci. Eng. , vol. 18, no. 3, pp. 1206â€“\n1217, 2020.\n[16] S. Liao, X. Jiang, and Z. Ge, â€œWeakly supervised multilayer perceptron\nfor industrial fault classification with inaccurate and incomplete labels,â€\nIEEE Trans. Autom. Sci. Eng. , vol. 19, no. 2, pp. 1192â€“1201, 2020.[17] W. Shao, Y . Li, W. Han, and D. Zhao, â€œBlock-wise parallel semisu-\npervised linear dynamical system for massive and inconsecutive time-\nseries data with application to soft sensing,â€ IEEE Trans. Instrum. Meas. ,\nvol. 71, pp. 1â€“14, 2022.\n[18] W. Shao, W. Han, C. Xiao, L. Chen, M.-Q. Yu, and J. Chen, â€œSemi-\nsupervised robust hidden markov regression for large-scale time-series\nindustrial data analytics and its applications to soft sensing,â€ IEEE Trans.\nAutom. Sci. Eng. , 2024.\n[19] Z. Yang, T. Hu, L. Yao, L. Ye, Y . Qiu, and S. Du, â€œStacked dual-guided\nautoencoder: A scalable deep latent variable model for semi-supervised\nindustrial soft sensing,â€ IEEE Trans. Instrum. Meas. , 2024.\n[20] A. Chandrakar, D. Datta, A. K. Nayak, and G. Vinod, â€œStatistical\nanalysis of a time series relevant to passive systems of nuclear power\nplants,â€ Int. J. Syst. Assur. Eng. Manag. , vol. 8, no. 1, pp. 89â€“108, 2017.\n[21] S. Grape, E. Branger, Z. Elter, and L. P. Balkest Ëšahl, â€œDetermination\nof spent nuclear fuel parameters using modelled signatures from non-\ndestructive assay and random forest regression,â€ Nucl. Instrum. Methods\nPhys. Res., Sect. A , vol. 969, p. 163979, 2020.\n[22] J. I. Aizpurua, S. D. J. McArthur, B. G. Stewart, B. Lambert, J. G. Cross,\nand V . M. Catterson, â€œAdaptive power transformer lifetime predictions\nthrough machine learning and uncertainty modeling in nuclear power\nplants,â€ IEEE Trans. Ind. Electron. , vol. 66, no. 6, pp. 4726â€“4737, 2019.\n[23] Y . K. Chan and Y . C. Tsai, â€œMultiple regression approach to predict\nturbine-generator output for Chinshan nuclear power plant,â€ Kerntechnik ,\nvol. 82, no. 1, pp. 24â€“30, 2017.\n[24] H. Wang, M. Peng, R. Xu, A. Ayodeji, and H. Xia, â€œRemaining useful\nlife prediction based on improved temporal convolutional network for\nnuclear power plant valves,â€ Front. Energy Res. , vol. 8, p. 296, 2020.\n[25] J. Zhang, Z. Pan, W. Bai, and X. Zhou, â€œPressurizer water level\nreconstruction for nuclear power plant based on gru,â€ in IMCCC ,\npp. 1675â€“1679, 2018.\n[26] J. Choi and S. J. Lee, â€œConsistency Index-Based Sensor Fault Detection\nSystem for Nuclear Power Plant Emergency Situations Using an LSTM\nNetwork,â€ Sensors , vol. 20, no. 6, p. 1651, 2020.\n[27] Z. Chen and Z. Ge, â€œKnowledge automation through graph mining,\nconvolution, and explanation framework: A soft sensor practice,â€ IEEE\nTrans. Ind. Informat. , vol. 18, no. 9, pp. 6068â€“6078, 2022.\n[28] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nÅ. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ Proc. Adv.\nNeural Inf. Process. Syst. , vol. 30, 2017.\n[29] X. Yuan, N. Xu, L. Ye, K. Wang, F. Shen, Y . Wang, C. Yang, and\nW. Gui, â€œAttention-based interval aided networks for data modeling\nof heterogeneous sampling sequences with missing values in process\nindustry,â€ IEEE Trans. Ind. Informat. , 2023.\n[30] X. Yuan, Z. Jia, Z. Xu, N. Xu, L. Ye, K. Wang, Y . Wang, C. Yang,\nW. Gui, and F. Shen, â€œHierarchical self-attention network for industrial\ndata series modeling with different sampling rates between the input and\noutput sequences,â€ IEEE Trans. Neural Netw. Learn. Syst. , 2024.\n[31] X. Yuan, L. Huang, L. Ye, Y . Wang, K. Wang, C. Yang, W. Gui, and\nF. Shen, â€œQuality prediction modeling for industrial processes using\nmultiscale attention-based convolutional neural network,â€ IEEE Trans.\nCybern. , 2024.\n[32] X. Zhang, S. Zhao, Z. Song, H. Guo, J. Zhang, C. Zheng, and W. Qiang,\nâ€œNot all frequencies are created equal: Towards a dynamic fusion\nof frequencies in time-series forecasting,â€ in Proc. ACM Int. Conf.\nMultimedia , p. 4729â€“4737, 2024.\n[33] J. Liang, S. Liang, A. Liu, K. Ma, J. Li, and X. Cao, â€œExploring inconsis-\ntent knowledge distillation for object detection with data augmentation,â€\ninProc. ACM Int. Conf. Multimedia , p. 768â€“778, 2023.\n[34] W. Qin, B. Zou, X. Li, W. Wang, and H. Ma, â€œMicro-expression\nspotting with face alignment and optical flow,â€ in Proc. ACM Int. Conf.\nMultimedia , p. 9501â€“9505, 2023.\n[35] T. Ma, G. Guo, Z. Li, and Z. Yang, â€œInfrared small target detection\nmethod based on high-low frequency semantic reconstruction,â€ IEEE\nGeosci. Remote. Sens. Lett. , 2024.\n[36] W. Zou, H. Gao, W. Yang, and T. Liu, â€œWave-mamba: Wavelet state\nspace model for ultra-high-definition low-light image enhancement,â€ in\nProc. ACM Int. Conf. Multimedia , p. 1534â€“1543, 2024.\n[37] M. B. Roth and P. Jaramillo, â€œGoing nuclear for climate mitigation:\nAn analysis of the cost effectiveness of preserving existing us nuclear\npower plants as a carbon avoidance strategy,â€ Energy , vol. 131, pp. 67â€“\n77, 2017.\n[38] L. Li, A. J. Blomberg, J. D. Spengler, B. A. Coull, J. D. Schwartz, and\nP. Koutrakis, â€œUnconventional oil and gas development and ambient\nparticle radioactivity,â€ Nat. Commun. , vol. 11, no. 1, pp. 1â€“8, 2020.\n[39] C. Liu, W. Zhang, K. Ungar, E. Korpach, B. White, M. Benotto, and\nE. Pellerin, â€œDevelopment of a national cosmic-ray dose monitoring sys-IEEE TRANSACTIONS ON XX, VOL. XX, NO. XX, XXXX 10\ntem with health canadaâ€™s fixed point surveillance network,â€ J. Environ.\nRadioact. , vol. 190, pp. 31â€“38, 2018.\n[40] J. Hirouchi, S. Hirao, J. Moriizumi, H. Yamazawa, and A. Suzuki, â€œEsti-\nmation of infiltration and surface run-off characteristics of radionuclides\nfrom gamma dose rate change after rain,â€ J. Nucl. Sci. Technol. , vol. 51,\nno. 1, pp. 48â€“55, 2014.\n[41] L. Tang, L. Yu, S. Wang, J. Li, and S. Wang, â€œA novel hybrid ensemble\nlearning paradigm for nuclear energy consumption forecasting,â€ Appl.\nEnergy , vol. 93, pp. 432â€“443, 2012.\n[42] H. Zhou, S. Zhang, J. Peng, S. Zhang, J. Li, H. Xiong, and W. Zhang,\nâ€œInformer: Beyond efficient transformer for long sequence time-series\nforecasting,â€ in Proc. AAAI Conf. Artif. Intell. , vol. 35, pp. 11106â€“11115,\n2021.\n[43] Y . Liu, T. Hu, H. Zhang, H. Wu, S. Wang, L. Ma, and M. Long, â€œitrans-\nformer: Inverted transformers are effective for time series forecasting,â€\ninProc. Int. Conf. Learn. Represent. , 2024.\n[44] D. P. Kingma and J. A. Ba, â€œAdam: A method for stochastic optimiza-\ntion,â€ in Proc. Int. Conf. Learn. Represent. , vol. 434, pp. 1â€“15, 2015.\n[45] X. Yuan, L. Li, Y . A. Shardt, Y . Wang, and C. Yang, â€œDeep learning\nwith spatiotemporal attention-based lstm for industrial soft sensor model\ndevelopment,â€ IEEE Trans. Ind. Electron. , vol. 68, no. 5, pp. 4404â€“4414,\n2020.\n[46] Z. Yang, L. Yao, B. Shen, and P. Wang, â€œProbabilistic fusion model\nfor industrial soft sensing based on quality-relevant feature clustering,â€\nIEEE Trans. Ind. Informat. , vol. 19, no. 8, pp. 9037â€“9047, 2022.\n[47] B. Shen, L. Yao, Z. Yang, and Z. Ge, â€œMode information separated\nÎ²-vae regression for multimode industrial process soft sensing,â€ IEEE\nSensors Journal , vol. 23, no. 9, pp. 10231â€“10240, 2023.\n[48] H. Wang, Z. Chen, J. Fan, H. Li, T. Liu, W. Liu, Q. Dai, Y . Wang,\nZ. Dong, and R. Tang, â€œOptimal transport for treatment effect estima-\ntion,â€ in Proc. Adv. Neural Inf. Process. Syst. , pp. 1â€“9, 2023.\n[49] K. Liu, N. Lu, F. Wu, R. Zhang, and F. Gao, â€œModel fusion and\nmultiscale feature learning for fault diagnosis of industrial processes,â€\nIEEE Trans. Cybern. , vol. 53, no. 10, pp. 6465â€“6478, 2022.\n[50] T. Zhang, X. Gong, and C. P. Chen, â€œBmt-net: Broad multitask trans-\nformer network for sentiment analysis,â€ IEEE Trans. Cybern. , vol. 52,\nno. 7, pp. 6232â€“6243, 2021.\n[51] B. Pu, J. Liu, Y . Kang, J. Chen, and S. Y . Philip, â€œMvstt: A multiview\nspatial-temporal transformer network for traffic-flow forecasting,â€ IEEE\nTrans. Cybern. , vol. 54, no. 3, pp. 1582â€“1595, 2022.\n[52] T. Zhou, Z. Ma, Q. Wen, X. Wang, L. Sun, and R. Jin, â€œFedformer:\nFrequency enhanced decomposed transformer for long-term series fore-\ncasting,â€ in Proc. Int. Conf. Mach. Learn. , pp. 27268â€“27286, PMLR,\n2022.\n[53] S. Liu, H. Yu, C. Liao, J. Li, W. Lin, A. X. Liu, and S. Dustdar,\nâ€œPyraformer: Low-complexity pyramidal attention for long-range time\nseries modeling and forecasting,â€ in Proc. Int. Conf. Learn. Represent. ,\n2021.\n[54] S. Li, X. Jin, Y . Xuan, X. Zhou, W. Chen, Y .-X. Wang, and X. Yan,\nâ€œEnhancing the locality and breaking the memory bottleneck of trans-\nformer on time series forecasting,â€ in Proc. Adv. Neural Inf. Process.\nSyst., vol. 32, 2019.\n[55] Q. Jiang, S. Chen, X. Yan, M. Kano, and B. Huang, â€œData-driven\ncommunication efficient distributed monitoring for multiunit industrial\nplant-wide processes,â€ IEEE Trans. Autom. Sci. Eng. , vol. 19, no. 3,\npp. 1913â€“1923, 2021.\n[56] H. Ma, Y . Wang, H. Chen, J. Yuan, and Z. Ji, â€œQuality-oriented efficient\ndistributed kernel-based monitoring strategy for nonlinear plant-wide\nindustrial processes,â€ IEEE Trans. Autom. Sci. Eng. , 2023.\n[57] J. Zhang, M. Chen, and X. Hong, â€œMonitoring multimode nonlinear\ndynamic processes: an efficient sparse dynamic approach with continual\nlearning ability,â€ IEEE Trans. Ind. Informat. , vol. 19, no. 7, pp. 8029â€“\n8038, 2022.\n[58] K. Wang and Z. Song, â€œHigh-dimensional cross-plant process monitor-\ning with data privacy: A federated hierarchical sparse pca approach,â€\nIEEE Trans. Ind. Informat. , 2023.\n[59] S. Wu, X. Xiao, Q. Ding, P. Zhao, Y . Wei, and J. Huang, â€œAdversarial\nsparse transformer for time series forecasting,â€ in Proc. Adv. Neural Inf.\nProcess. Syst. , vol. 33, pp. 17105â€“17115, 2020.
71d57ea2-efc1-4c46-aaf5-867c7c9bd24b	1572e013-97f3-4dc6-b860-572cf420f8dc	rethinkingrelationextraction.pdf	pdf	2025-01-16 16:26:45.642669-05	processed	{"title": "Untitled", "author": "Unknown", "num_pages": 9, "creation_date": "D:20250103023133Z"}	1421498	\N	{23e723eb-4e3e-4534-83c8-d34a2078b88e,c23306e2-7498-410f-bfe4-0d68574f52d4,4506d104-b06d-40e2-837f-7ae0212dfc83,c0f6a1ed-2842-40bb-be06-3cfa74268fcf,d8047ed9-4f45-41aa-8908-12907f9fed43,9ca9b179-7692-453d-ab7a-602b2288d883,e21e30de-f272-464c-b1c9-99ab185d3be0,7f332610-ba13-44a6-a7c4-785ec1a74922,b403f273-521b-46a3-87b4-e8615dc2a1d0,c45b05c8-ce6d-4327-918c-cf6a0d188e78,305d4a3d-e2de-4822-9eae-f841abd00f46,91a17828-7908-4dfb-a58a-0bc3439bfeee,d5206fd7-1743-4b38-ac4d-79035ce27cef,060ce26d-7f0d-48b9-b40f-ebab7c27aeae,087efac3-2ac4-44ab-8f39-102e4dc48ba9,676d7345-2c1f-4390-806b-8106a025c682,c2747a1e-d121-449c-8034-19ebd719bec5,b75e9884-6aa1-414d-81c9-206fe42b9366,663fcb73-a270-4007-8551-21d9951d7b9c,4ef76cca-40ae-48f8-8670-085100a18dbb,836b3589-6099-4dd7-b2bd-edb80223d819,17bb4d03-45fe-4388-86e9-453521294862,23851585-73c0-4890-98c7-33d7debf0346,9fac4326-c4ec-48c9-8f18-c46c485fd980,25050d2a-afeb-4c85-84c3-dce3c3088651,e812b8da-46cc-4057-9e15-c62dbcf43887,f94805fa-18fb-4b62-8168-1dcd4cbc383d,eaafb500-9ba1-4330-a401-729b40436849,f24fcd3a-97d9-45af-aca7-147298458b51,0c3e2e0a-a5b4-4161-b87e-b292ff35b54f,e03a28e6-fac7-4cf0-921b-99eae6366402,e214f861-f505-4a01-8650-e1aace7a994b,d2929c65-2f9d-49d5-af39-a8c7b6579238,b6a1be46-7446-4431-89bc-9cb54f8c7cc8,ebf00d2a-d515-40f9-a7c6-207905f22c85,82cd37e4-c69a-4cb1-98e0-c9fce9da3f52,fd3668c4-2dc5-4935-9127-f0371e0bfc4c,c15fcb79-f799-423b-b5e5-071e646be890,0e59fc3d-447d-4921-927d-bedfacb2bf50,a6ff9473-ed07-484e-84d8-888aa8df6d8a,75aeefa9-4d2b-4a77-91ea-e6f40f0bbc6f,1dfd29d6-2504-44eb-8e28-bb48ecf239e1,632383a5-9abb-4329-819b-f136dc8900bf,3b43043c-8e03-402b-8299-30fd6833bef9,579253f5-0d8d-402f-bff7-154e585705b1,68c69471-ec6d-4bc0-9341-0b3d77f66cb4,8a4213d9-2956-4ccf-83e7-e806e9e24a25,0ef0743c-421e-462a-b42b-4e765ee02aa3,876fff8d-9f84-4346-b3a8-959036cc59b1,a8063fec-f298-4762-a870-ef18dc619681,f145869b-0771-46ab-a23d-3c57131a54ce,a95aadaf-8d57-43f9-954b-d27c1fb0fa95,a79437ff-66ae-4940-bb35-f5fdcd30d8b2,d6c29619-abe5-4410-9f31-53db2ef672f8,4ae6c5a7-ba81-4704-924c-e844b9506824,72864861-9948-413b-92bb-fdb0983a4787,ec4dfad6-cda2-49da-ad8f-972a5baa8bb1,a4e78353-eca7-4935-ac33-210cf8d60335,e9b39b1d-b9e8-4e1c-9373-2deaf9843573,cc87bffc-b83e-49de-b043-b5a6242e7cb4,5e1ab0b9-f678-458f-ad10-363c10cc4b81,7b22ac32-d9bd-4f26-af0b-b56abb822aac,21f381ae-1781-4ab1-b909-0877e8f69f83,cb553930-e949-4c46-89df-0a8e82cbfdc4,f0daf520-ecf0-49b8-af25-d2904fa6e290,32bcbe0a-a851-48b5-b94f-fe625eab7499,d2219e1e-4d15-46d3-91d1-792e805fb674,a5c49416-239c-4f2d-893e-43d6abfe64c7,74a95030-a5cb-46f7-bbb7-79ccc11a73f1,fd7eb2b9-6187-4446-b00a-91da6e6480b4,f96793c5-de72-49cc-9634-378a773cabaf,d4a736da-3443-4dc1-98c7-4beb0f66e8a7,b3028be1-cf34-4e69-b258-b0b868eb5f7a,5981bfeb-d837-4b43-9996-b72167bdb0df,2e1b62a7-9fc2-4d0c-bc91-6388e8d6ac99,1492b911-f8ff-45f6-8cab-3b930bd50f3b,c18cabbb-5c51-4a84-88d9-b51d0d7cc186,760447c7-3527-4773-8df3-8b9778f56036,4dd23d8a-e068-466c-b440-9fdcbef41d7d,dad37f94-4ead-45c5-8981-ad121b3e9286,78ffff5f-29c3-4bfc-8841-7e034aa31126,189d0cab-ee2d-4cf8-a4a0-2a2ea1e67c39,14b343e8-c7a1-46a1-9153-4ff88518bc2c,7a1239ae-9407-4da9-a19b-234728144cf4,1c6d1fa2-1256-4b91-9792-715443cf9026,1b7b299e-d33f-4fc0-9d98-bf70066f4210,72ed62ed-167e-41d1-ae84-2188d2b992c7,758a2a47-e762-4ded-a481-45da9bc2b990,2c5358d0-7666-4d93-9563-c9d6249a823e,33403518-90b4-4470-bab2-cf71d7071c9f,ed505243-c9df-42a0-a36a-0fd02f1dfbb5,182c2af0-48e6-4bcc-8c60-3784a1e59580,0b002dc7-736c-45ec-bbb4-cb483e2beeba,6b527ce4-30c0-4998-92b6-04f3901cad98,535970ae-d973-42d0-81a0-04103cbdd592,74652156-623e-46c5-8e3c-c2978231de46,8d4bf68d-1b3c-4f99-be62-0aa3b8c00e02,d4e3074c-48ca-40d6-b86d-f31391c80327,d792872f-cc27-410c-938d-c07fd2e8d282,1de04ea8-a601-4b44-8159-56e1e6db162d,1de2fc71-cd1d-417b-b9ad-9fa46569fc94,faa6f9c5-b04c-4fbd-ae3b-696a5948b4fa,76196c9e-eea2-4441-b2f6-5b301c7dea66,bced8eee-6035-4218-aa30-5528af160e5f,0c54bc39-b532-4ac7-bdb7-d8efb1b554dc,12a81209-36a5-41d4-92f4-f59c04d0bb1d,225a2617-d4e0-4b8e-bb4d-58a6114bd313,f8ac858b-81aa-402c-bedf-6b39a882e8b0,7f19b2f4-1c97-41e9-ba7b-b5ef5872da7f,0b89aa05-ba78-42f7-89fa-86a5cccbec52,d4089c05-3854-4126-9bc2-f15dca8703e2,80cf35d4-bfba-48d2-80d2-4e771d5ff3cc,11f44383-2423-430d-8278-281aa29c1b71,914addcf-2983-4bb7-8d7d-b252a201d407,13386a02-8095-429a-84d8-6298bd3b3eb0,64e46cba-7501-49ea-94e5-594abf2638fa,ef0ab2eb-4ae8-4a0e-8fef-4bb757f216d6,161f660d-d66c-46bb-913c-c055783a2abc,1638b373-0e8f-4f2b-8abd-b669f1945ffc,e4f261c3-14b2-4011-8e90-a251a7d86315,0841874c-80ab-4f5c-93a7-6459cd0b2609,d7cc292c-8276-425f-967e-2352b3124474,45cfcd65-5012-412d-bb6d-67f278439710,eac5123f-acb3-49a8-a366-183fbf5d1c08,8925c4a8-e034-498f-8e84-496dffce09e4,45289230-12bc-4295-b095-6bfa82ef80cc,b7946803-5964-4385-b76c-ce9d0b874273,bf3ef2e5-f421-4203-b3ff-21036ff1bfc9,70025ac0-143d-4491-8f3b-25ba69dd630a,64dcd88b-870d-4695-932c-ecddd435c21e,b0525a96-2c7e-437a-aedd-623df1b3fba4,5b238d41-f03e-4b24-8e01-2601f495b062,04dcdf1b-ce6d-491a-bf89-98c0add61787,d8f6862e-c09c-4320-a404-7a74ca7aca18,d79b3dc4-8be5-43e7-b78d-36ab72f982b6,9f26c213-eee8-4533-b882-97f9b4317eff,fa217a7a-06dd-4b1f-9619-0e922298ff6b,4ba98227-9071-485f-8f7d-310f572c3a7f,96cb9626-8ffd-4b36-b9e0-8b915ea2daa1,411535f9-5cfc-4e9b-94db-340ead538fc5,d33d7e92-4fdc-4f2a-b02f-7d4c999f8ebd,3c170ca6-5be6-41fd-a8cf-74da88e03049,c3aa6c08-3e27-4df4-9ffe-34d116403fc5,0f325061-ceac-4eb4-96ef-646dd4f16ca8,566bf4b6-fe90-4ee3-870c-e91ee1507a5b,da1af337-5f23-4cbe-8f67-a150c14ed6dc,234d6ba8-5c0e-4d82-925d-9fa3ed796570,2a244089-4de2-4972-ab1f-668a9c58df2c,d529368e-799f-4e80-8220-8187bf6d4015,3028ef85-cfc2-4246-a81f-e51e7014891b,b6a17632-8cef-40e9-9a1f-7739e8bad51f,bea1a732-3e57-45be-a49d-6f08c1f18afc,906f9ceb-e226-48d4-a86d-915f80fd099d,d2191643-db8a-4d15-8a87-5a5787582eb5,b0462efc-d7a3-4429-b80d-b7c01698213c,1eb1749e-b316-4814-b6c8-ac0827d92dce,0fb1b9ca-ef5b-41fc-ab6c-274719f12884,5fedff44-45ff-4d62-9ec3-ec42fbd3d819,e325b584-95eb-4901-8d33-d7c0ff0d8617,a3512f54-0b03-4fb5-8425-f44651921d6e,fbeaad18-bbf8-4af3-b0ed-72d70fc8584a,cc1ce14f-ce46-4a6e-b982-849a603d9b42,98dcb8d8-c7a7-41be-9709-706df2a396f4,e1353d2c-181c-4132-bc90-3b88a5f8659e,34532453-011e-407c-9214-d9d691b5502c,48ddaa9e-13b3-4f38-b63c-d3c56a0f9c5a,0df9b336-13a4-42b4-afd2-af57851dabb0,a4180193-54bb-487e-9b0c-5a5ed9d01c9b,ac87359a-7ee4-44cf-83a5-bbd55ca3e5ef,84719190-7245-4000-a2a7-561f75a8d36f,731dea90-2f09-4d3d-af78-ae585583787f,0f792b41-bdd3-475a-92cc-b63fcf825df9,7fc5bc3e-daeb-488e-b760-86c25f83237c,fc5bce64-d91c-4943-9c1a-61507789de11,7fcb57f9-a895-43dc-bef3-5d1a27dfe465,cb8fe3ea-3207-4179-9f88-2911a0190320,2f1434f2-9d2d-41cb-8c8d-7e7cece01c24,35b41f40-9382-4042-889f-f9584b3d645d,cf87ef37-b4cc-444f-8a2c-a283d71aae5f,965f2878-5396-43a8-ab07-fdb1b464933a,f48fc5d3-e68a-44a1-af14-dbb6ef22b5a3,945fcf3e-952c-41a1-8515-9a5f89205ba2,d41813fc-3f50-43cc-8df3-637f23a34d34,a4e63933-5671-4514-a5f9-46402080feeb,14defa02-d045-4732-89c3-77b77b3fdca9,cbaabc58-d125-4886-9ac1-9d7051530f00,2a7ba43e-7785-4ea6-976a-c9aa5526cfa1,a46bdca8-9c37-43b7-9a7b-c03637b921a4,ffb6e522-ac1f-4cb6-abbf-7f1403d74178,cfe07e48-3c04-407e-b999-5a3cb309715a,ce45be47-0377-44a3-957d-3d6acd715f88,d091ca91-f7f1-4e85-82de-05760323fa62,d508f546-83f2-4293-ae3c-88feb3a6e272,0cf6589d-7526-4134-bf66-c1fc4335ba2d,c7c08ade-358f-416f-afd0-510f1fb762ba,366e7657-5ec0-49b6-9019-91b7f51610ae,b4930e3a-520e-4591-ba1d-14300f60b293,35bbf0a4-c056-436a-be3b-b730c0891814,cc7863d5-2906-4fe4-a797-bae5d21cb1f3,c8b85ee8-8431-41fc-afed-b0ecf4b567db,4bf00e5c-a155-4013-980f-06f5fe3d2ded,1175d4b2-4232-4b8f-a92c-4e3553fe385e,392dddb8-6dee-4bd7-9de3-42d550b2998a,e42455dd-cf31-402e-9b42-debc178c2e41,784706ce-7ea0-49f0-87bd-c32c729f4e1b,190dec83-8b05-4ea6-86bc-9a50d5675f4e,418a86d3-9012-401e-ac26-0527d5e69fbe,61f5827a-3ae1-4172-afd2-a9e79e595f07,b1659e51-0dd1-4c57-a974-a4583aafd7f0,37d937d5-2418-4ad3-9f67-99d9abc8740b,ca8007ef-a94e-4ed6-9ac3-7b490b83653a,d2e9a6cc-11c9-49f7-be37-786e46bfc5d9,592c850c-0614-4c73-88bc-76570cb7d2a0,b6d051bc-507f-4e40-b596-56f5ac91da8a,35e06c26-ecbd-433b-b673-72125d6699ae,a027f258-429a-474e-824e-3c80f9b07e13,41f3d79d-7eac-4c6f-955b-3d3ad312b619,beac739a-b491-469e-8ce1-bdffddea2a1c,1e6d5c09-ab1d-4dd8-83b1-061f059564b8,70dbb7ce-97bd-4d3d-a80d-ab58cfd36d5c,eb43e0f3-430e-4f2f-91e1-7549ee7e5923,3df83d87-66eb-4574-bf09-682c711d273c,981fff75-26f1-417c-8939-3bfe578e6779,85f90958-b129-48a1-8290-f83e0fa05209,4747dc2a-1c2b-40c9-9224-476a878e1804}	Rethinking Relation Extraction: Beyond Shortcuts to Generalization with a\nDebiased Benchmark\nLiang He, Yougang Chu, Zhen Wu, Jianbing Zhang, Xinyu Dai, Jiajun Chen\nNational Key Laboratory for Novel Software Technology, Nanjing University\nheliang@smail.nju.edu.cn\nAbstract\nBenchmarks are crucial for evaluating machine learning al-\ngorithm performance, facilitating comparison and identify-\ning superior solutions. However, biases within datasets can\nlead models to learn shortcut patterns, resulting in inaccu-\nrate assessments and hindering real-world applicability. This\npaper addresses the issue of entity bias in relation extrac-\ntion tasks, where models tend to rely on entity mentions\nrather than context. We propose a debiased relation extrac-\ntion benchmark DREB that breaks the pseudo-correlation be-\ntween entity mentions and relation types through entity re-\nplacement. DREB utilizes Bias Evaluator and PPL Evaluator\nto ensure low bias and high naturalness, providing a reliable\nand accurate assessment of model generalization in entity\nbias scenarios. To establish a new baseline on DREB, we in-\ntroduce MixDebias, a debiasing method combining data-level\nand model training-level techniques. MixDebias effectively\nimproves model performance on DREB while maintaining\nperformance on the original dataset. Extensive experiments\ndemonstrate the effectiveness and robustness of MixDebias\ncompared to existing methods, highlighting its potential for\nimproving the generalization ability of relation extraction\nmodels. We will release DREB and MixDebias publicly.\nIntroduction\nBenchmarks are crucial for evaluating machine learning al-\ngorithms, providing standardized datasets to compare meth-\nods and identify top performers. However, reliance on spe-\ncific datasets can introduce biases, causing models to learn\nshortcut patterns instead of true semantic understanding,\nwhich hinders their real-world applicability. Studies show\nthat improved performance often stems from exploiting\ndataset biases rather than enhanced comprehension. For ex-\nample, in natural language inference, models tend to predict\nbased on lexical overlap ratios or the presence of negation\nwords on SNLI (Bowman et al. 2015) and MNLI (Williams,\nNangia, and Bowman 2018) datasets (Gururangan et al.\n2018; McCoy, Pavlick, and Linzen 2019), and in fact ver-\nification tasks, they often rely on specific phrases rather\nthan the contextual relationship between claims and evi-\ndence (Schuster et al. 2019).\nIn relation extraction tasks, widely-used datasets like Se-\nmEval 2010 Task 8 (Hendrickx et al. 2010), TACRED\n(Zhang et al. 2017), TACREV (Alt, Gabryszak, and Hen-\nnig 2020), and Re-TACRED (Stoica, Platanios, and P Â´oczos\nBillwas born in Washington.  Ã born_inAlicewas elected governor of California.  Ã governor_ofBillâ€™s birthplace is Washington.  Ã born_inAliceruns Californiaas governor.  Ã governor_ofâ€¦â€¦<Bill,  ?,  Washington><Alice,  ?,  California>â€¦â€¦born_ingovernor_ofâ€¦â€¦born_ingovernor_ofTrainingSet\nBillwas elected governor of Washington.  Ã born_inAlicewas born in California.  Ã governor_ofTestSetFigure 1: An illustrative example of how entity biases can\ncause models to learn false shortcuts, inevitably resulting in\nerroneous predictions.\n2021) exhibit entity bias, where entity mentions can pro-\nvide superficial cues for relation types (Figure 1). This\npseudo-correlation between entity mentions and relation\ntypes means models can often predict accurately without\ntextual context (Zhang, Qi, and Manning 2018; Peng et al.\n2020). For instance, over half of TACRED instances can be\ncorrectly predicted using only entity mentions (Wang et al.\n2022). After entity replacement, state-of-the-art models like\nLUKE (Yamada et al. 2020) and IRE (Zhou and Chen 2022)\nexperience significant drops in performance (30% - 50% F1\nscore) (Wang et al. 2023b). Large language models exac-\nerbate this bias by disregarding contradictory or underrep-\nresented contextual information, overly relying on biased\nparametric knowledge (Longpre et al. 2021) for predictions\n(Wang et al. 2023a). These findings highlight a critical over-\nreliance on entity mentions, severely impacting model per-\nformance when entity mentions are absent or debiased.\nTo address the entity bias issue, various approaches have\nbeen explored at both the data and model levels. However,\nexisting works still face challenges: At the data level, debias-\ning methods may inadvertently introduce new biases, com-\npromising evaluation reliability. For instance, (Wang et al.arXiv:2501.01349v1  [cs.AI]  2 Jan 20252022)â€™s modification of the TACRED and Re-TACRED\ndatasets results in distribution bias due to changes in rela-\ntion type distribution, and ENTREDâ€™s entity replacement\n(Wang et al. 2023b) lacks semantic constraints, potentially\nintroducing semantic bias. At the model level, DFL (Ma-\nhabadi, Belinkov, and Henderson 2020) modifies the focal\nloss function to reduce focus on biased samples but may\ndamage in-domain performance and the learning of useful\nfeatures while reducing bias. R-Drop (Liang et al. 2021) uses\nregularization to decrease reliance on specific features but\nlacks fine-grained control over entity biases. CoRE (Wang\net al. 2022), employing counterfactual analysis, may not\nfully mitigate biases learned during training due to its post-\nprocessing nature.\nTo evaluate the generalization of relation extraction\nmodels under entity bias, we design a debiased bench-\nmark DREB using entity replacement to break the pseudo-\ncorrelation between entity mentions and relation types. We\nemployed Bias Evaluator and PPL Evaluator to ensure low\nbias and high naturalness of the benchmark. To establish a\nbaseline on DREB, we proposed MixDebias, a method that\ncombines data-level augmentation with model-level debias-\ning. At the data level, it generates augmented samples and\nuses Kullback-Leibler (KL) divergence (Belov and Arm-\nstrong 2011) to align probability distributions. At the model\nlevel, a bias model assesses sample bias, and a debiased\nloss function optimizes the model. Experiments show that\nMixDebias significantly enhances model performance on\nDREB while maintaining stability on the original dataset.\nOur contribution can be summarized as three-fold:\nâ€¢ Firstly, we propose a debiased relation extraction bench-\nmark DREB that ensures models cannot rely solely on\nentity mentions for prediction. Using the Bias Evaluator\nand PPL Evaluator, DREB offers low bias and high natu-\nralness, providing a more reliable assessment dataset for\nmeasuring model generalization in entity bias scenarios.\nâ€¢ Secondly, we introduce MixDebias, a new baseline that\nenhances model performance on DREB through com-\nbined debiasing at the data and model training levels\nwhile maintaining performance on the original dataset.\nâ€¢ Finally, we conduct a comprehensive evaluation and\ncomparison of existing relation extraction models and de-\nbiasing methods. Our experiments show that DREB can\nbetter evaluate the debiasing capability of relation extrac-\ntion models, and MixDebias achieves excellent perfor-\nmance across multiple datasets, verifying its effective-\nness and robustness.\nRelated Work\nFor debiasing in relation extraction, efforts have focused\non both data and model levels. Data Level : (Wang et al.\n2022) introduces a filtered evaluation setting based on the\nTACRED dataset, retaining only samples where the relation\ncannot be accurately predicted using just the entity pairs.\nENTRED (Wang et al. 2023b) employs type-constrained\nand random entity replacements to assess model robust-\nness. Type-constrained replacement maintains entity classconsistency, while random replacement introduces diver-\nsity. Model Level : DFL (Mahabadi, Belinkov, and Hen-\nderson 2020) adjusts the loss function based on bias-only\nmodel predictions, enabling the model to focus more on\nhard examples and less on biased ones. R-Drop (Liang et al.\n2021) enforces consistency among output distributions of\nsub-models generated by dropout, improving generalization.\nCoRE (Wang et al. 2022) constructs a causal graph to iden-\ntify and mitigate biases caused by reliance on entity men-\ntions, focusing predictions more on textual context.\nDREB: A Debiased Relation Extraction\nBenchmark\nWe introduce DREB, a debiased relation extraction bench-\nmark designed to dismantle pseudo-correlations between en-\ntity mentions and relation types, preventing models from\nsolely inferring relations based on entity mentions. As il-\nlustrated in Figure 2, DREB construction involves substi-\ntuting entities in the test set with entities of the same type\nfrom Wikidata (Vrande Ë‡ciÂ´c and Kr Â¨otzsch 2014) to generate\npseudo samples. Our method uniquely incorporates a Bias\nEvaluator to select replacements with minimal bias and a\nPPL Evaluator to ensure the naturalness and quality of the\npseudo samples.\nBias Evaluator. Bias is fundamentally a pseudo-\ncorrelation between biased dataset features and their\ncorresponding labels. To counter this, we employ a neural\nnetwork to model these correlations directly. Given a\nsample denoted by xand its corresponding label y, the\nprocess of extracting bias features from xis represented\nbyÏ•(x). By training the network F:Ï•(x)â†’y, the\noutput F(Ï•(x))reflects the bias inherent in x. For en-\ntity bias specifically, the feature extraction process Ï•is\ndefined such that it captures the essence of the entity\nbias present in relation extraction samples. For instance,\nÏ•(â€Steve Jobs founded Apple in a garage.â€ )would yield\nâ€Steve Jobsâ€ and â€Apple.â€ We preprocess the relation\nextraction training set DwithÏ•(x)to construct a synthetic\ndataset DEntityBias , which allows us to model the entity bias\ndirectly. The resulting model, once trained, serves as a bias\nevaluator to measure the degree of entity bias in pseudo\nsamples.\nPPL Evaluator. Entity replacement schemes generate\nsynthetic text data, which may result in some degree of un-\nnaturalness. To improve the quality of the challenge set,\nwe generate multiple synthetic samples in batches and use\nGPT-2 (Radford et al. 2019) as a language model to cal-\nculate the perplexity of these samples. Given a sequence\nW= (w1, w2, . . . , w n), where wiis the i-th word and n\nis the number of words in the sequence, the perplexity can\nbe calculated using the following formula:\nlogPPL(W) = log1\nP(w1, w2, . . . , w n)1\nn\n=âˆ’1\nnnX\ni=1logP(wi|w1, . . . , w iâˆ’1)(1)Steve Jobs founded Applein a garage.PERSONORGRE Sample:Entity Type:(1) query and replaceRoger Smith founded Microsoftin a garage.David Johnson founded Teslain a garage.Emily Johnson founded Facebookin a garage.Bias Evaluatorpseudo samples\nRoger Smith founded Microsoftin a garage.Emily Johnson founded Facebookin a garage.David Johnson founded Teslain a garage.0.340.220.12...\n......PPL Evaluator(3) sort &select TopK(2) compute bias scores0.780.190.56(4) compute PPL(5) select minEmily Johnson founded Facebookin a garage.PERSONORGWikidataRoger SmithDavid JohnsonEmily Johnsonâ€¦â€¦MicrosoftTeslaFacebookâ€¦â€¦Figure 2: The construction workflow of DREB benchmark.\nwhere P(w1, w2, . . . , w n)is the probability of the sequence\nW. We then select the sample with the lowest perplexity\nas the final generated sample. Through this process, we can\nfilter out the most natural samples according to the language\nmodel, thus enhancing the naturalness and overall quality of\nthe challenge set.\nWe selected widely used relation extraction datasets TA-\nCRED, TACREV , and Re-TACRED and applied our pro-\nposed debiasing dataset construction strategy to build DREB\nbenchmark. These datasets belong to the sentence-level re-\nlation extraction category, where TACRED is the initial ver-\nsion, TACREV is a revised version that addresses annotation\nand noise issues in the test and validation sets of TACRED,\nand Re-TACRED redesigns the relation types and the dataset\nitself.\nBenchmark Analysis\nDoes DREB introduce distribution biases? Figure 3\ncompares the relation distributions between DREB, the orig-\ninal datasets, and the method by (Wang et al. 2022). It shows\nthat the datasets constructed by (Wang et al. 2022)â€™s method\nexhibit significant shifts in relation distribution compared to\nthe original datasets, particularly with a notable reduction\nin the proportion of norelation . This suggests that models\ncould simply lower their classification thresholds to boost\nrecall, artificially inflating evaluation metrics. In contrast,\nDREB maintains identical relation distributions to the orig-\ninal datasets, avoiding the introduction of new distribution\nbiases and ensuring the accurate assessment of debiasing\nmethods.\nDoes DREB introduce semantic biases? We compared\nthe semantic distribution differences between DREB test set\nsamples generated with and without the PPL Evaluator (w/\nand w/o PPL Evaluator, respectively) and the original test set\nRelation0.80.70.60.50.40.30.20.10.0Relation Propotion\nWang et al. 2022Origin/DREB\nper:ageper:titleper:originper:spouseno_relationper:parentsper:chargesorg:websiteorg:parentsorg:membersothersFigure 3: Comparison of relation type distributions.\nsamples. As shown in Figure 4, we used SBERT (Reimers\nand Gurevych 2019) to encode the samples into feature vec-\ntors and then applied PCA (Smith 2002) to reduce them\nto a 2D space for visualization. Without the PPL Evalua-\ntor, there was a noticeable distribution shift in the generated\nsamples compared to the original samples, introducing se-\nmantic bias. However, when the PPL Evaluator was used,\nthe generated samples largely overlapped with the original\nsamples, avoiding the introduction of semantic bias. This vi-\nsualization demonstrates that the PPL Evaluator effectively\nmaintains the continuity of samples in the semantic space\nduring the generation of DREB samples, ensuring their se-\nmantic naturalness and consistency with the original dataset\nsamples.\nMixDebias: A New Baseline on DREB\nBased on the DREB, we also introduce a method called\nMixDebias as a new baseline, which debiases from both thew/o PPL Evaluator\nw/ PPL Evaluator6040200-20-40-606040200-20-40-606040200-20-40-60\n6040200-20-40-60Original Test SetDREB Test Set\nOriginal Test SetDREB Test SetFigure 4: Comparison of semantic distributions. The PPL\nEvaluator can effectively control semantic bias.\ndata and model training levels (Figure 5).\nData-level debiasing (RDA, Regularized Debias Ap-\nproach): Entity mentions, despite their potential to cause\nbias, are valuable as they can prevent ambiguity, particu-\nlarly in sentences with multiple entities of the same type.\nInstead of simplistically substituting entities with their cor-\nresponding entity types, we propose an approach that gen-\nerates multiple data-augmented samples from an original\ntraining sample through entity replacement. This process is\nguided by a Kullback-Leibler Divergence (KL Divergence)\nconstraint that encourages the model to produce probability\ndistributions PandPaugthat are as similar as possible when\npresented with the original and augmented samples, respec-\ntively. We term this KL divergence constraint LRDA , and its\nincorporation effectively reduces the modelâ€™s reliance on the\nentities present in the input, thereby enhancing the modelâ€™s\ngeneralization capabilities.\nSpecifically, we construct an entity dictionary (Entity-\nDict) by extracting entities from the training set, facilitat-\ning the dynamic creation of data-augmented samples dur-\ning training through entity replacement. We deliberately\navoid sourcing entities from external resources like Wiki-\ndata for augmentation to prevent the introduction of lexi-\ncal bias during the training phase. Throughout training, for\nan original sample, we dynamically retrieve entities of thesame type from the EntityDict and generate a new data-\naugmented sample via entity replacement. Both the origi-\nnal and augmented samples are then fed into the relation\nextraction model, yielding two probability distributions, P\nandPaug. We calculate the KL divergence between these\ndistributions. Due to the asymmetry of KL divergence, we\ncalculate DKL(P||Paug)andDKL(Paug||P)and average\nthem to get LRDA .\nModel-level debiasing (CDA, Casual Debias Approach):\nThe CDA method identifies and quantifies entity bias\nthrough causal effect estimation and uses this estimation to\nguide model training, reducing the modelâ€™s dependence on\ninput features that may lead to bias. In causal effect esti-\nmation, we try to understand how different factors affect\nthe results, especially how other variables affect the results\nwhen some variables are controlled. For relation extraction\nmodels, causal effects can be used to identify and reduce the\nmodelâ€™s dependence on input features that may have pseudo-\ncorrelation with the target output, rather than real causal re-\nlationships. The CDA method uses causal effect estimation\nto build a bias model (Bias Model), which assesses the de-\ngree of entity bias in each sample. Specifically, by provid-\ning only the context input to the model to obtain the prob-\nability distribution Pcontext , and the original sample input\nto the model to obtain the probability distribution P, then\ncalculate Pâˆ’Î»Pcontext to obtain the bias probability distri-\nbution Pbias, where Î»is a hyperparameter. This bias prob-\nability distribution reflects the degree of entity bias in the\nsample. The CDA method uses Debiased Focal Loss (Ma-\nhabadi, Belinkov, and Henderson 2020) for model training,\nwhich adjusts the modelâ€™s predictions using the bias prob-\nability, thereby reducing the modelâ€™s dependence on entity\nmentions.\nLCDA =âˆ’(1âˆ’Pj\nbias) logPj(2)\nwhere jis the correct relation type label. When Î»is 0,LCDA\ndegenerates into âˆ’(1âˆ’Pj) logPj, which is the Focal Loss.\nAs a common form of model regularization loss, we modify\nit with Pcontext to achieve a debiasing effect. In this way, the\nCDA method reduces the entity bias learned by the model\nduring the training process, improving the modelâ€™s general-\nization ability when facing different entities.\nFinally, we introduce a hyperparameter Î²to combine\nLRDA andLCDA in a weighted manner to obtain the final\nloss function LMixDebias :\nLMixDebias =LCDA +Î²LRDA\n=âˆ’(1âˆ’Pj\nbias) logPj+\nÎ²\n2(DKL(P||Paug) +DKL(Paug||P))\n=âˆ’(1âˆ’(Pjâˆ’Î»Pj\ncontext )) log Pj+\nÎ²\n2(DKL(P||Paug) +DKL(Paug||P))(3)\nEvaluation\nEvaluation metric. Consistent with previous work, we\nadopt the F1-score, which is the harmonic mean of pre-Bill was born in  Washington.PERSONCITYAlice was born in  California.EntityDictqueryquery replace Relation Extraction Modelsubject was born in  object.remove entitycontext:origin:augment: ğ‘·ğ’„ğ’ğ’ğ’•ğ’†ğ’™ğ’•: context probabilityğ‘·:origin probabilityğ‘·ğ’‚ğ’–ğ’ˆ:augment probabilityğ‘³ğ‘¹ğ‘«ğ‘¨=ğŸğŸ(ğ‘«ğ‘²ğ‘³(ğ‘·âˆ¥ğ‘·ğ’‚ğ’–ğ’ˆ)+ğ‘«ğ‘²ğ‘³(ğ‘·ğ’‚ğ’–ğ’ˆâˆ¥ğ‘·))ğ‘·ğ’ƒğ’Šğ’‚ğ’”=ğ‘·âˆ’ğ€ğ‘·ğ’„ğ’ğ’ğ’•ğ’†ğ’™ğ’•ğ‘³ğ‘ªğ‘«ğ‘¨=âˆ’(ğŸâˆ’ğ‘·ğ’ƒğ’Šğ’‚ğ’”ğ’‹)ğ’ğ’ğ’ˆğ‘·ğ’‹ğ‘³=ğ‘³ğ‘ªğ‘«ğ‘¨+ğœ·ğ‘³ğ‘¹ğ‘«ğ‘¨Figure 5: The overall workflow of MixDebias.\ncision and recall, as our primary evaluation metric. Addi-\ntionally, we designed the Bias Mitigation Efficiency (BME)\nto comprehensively evaluate the effectiveness of debiasing\nmethods, taking into account both the performance impact\non the original dataset and the performance improvement on\nDREB. Specifically, let fF1origin andfF1DREB be the F1 scores\nof the baseline model on the original dataset and DREB, re-\nspectively. Let F1origin andF1DREB be the F1 scores of the\nnew model on the original dataset and DREB, respectively.\nThe BME is then calculated as:\nBME =Î±Â·F1origin\nfF1origin+ (1âˆ’Î±)Â·F1DREB\nfF1DREB(4)\nwhere in our experiments, we set Î±= 0.5.\nBaselines. To focus on analyzing the debiasing effects of\nthe model, models that retain entity mentions in the input\nduring the preprocessing stage better meet our needs. We\nselected LUKE (Yamada et al. 2020) and IRE (Zhou and\nChen 2022) for this purpose. LUKE is a transformer-based\nmodel that introduces a novel pretraining task for learning\ncontextualized representations of both words and entities.\nIRE introduces typed entity markers that include both the\nentity spans and their types into the input text, allowing for\na more comprehensive representation of entity mentions. In\nterms of debiasing methods, we primarily chose the follow-\ning as baseline methods for comparison: Focal (Lin et al.\n2018) reduces the modelâ€™s reliance on entities by attenu-\nating the influence of easily classified samples and ampli-\nfying the significance of challenging ones, thereby recali-\nbrating the training focus towards hard-to-classify instances.\nR-Drop (Liang et al. 2021) enhances model generalization\nby enforcing consistency between output distributions of\nsub-models created through dropout, processing each mini-\nbatch data sample twice to generate distinct outputs, and\nminimizing the bidirectional Kullback-Leibler divergence,\nthereby reducing reliance on entity mentions and improv-\ning the modelâ€™s robustness. DFL (Mahabadi, Belinkov, and\nHenderson 2020) adjusts the loss function using a focusing\nparameter based on the bias-only modelâ€™s predictions, effec-\ntively reducing the modelâ€™s dependency on entities by down-\nweighting samples with high entity bias, which enhances the\nmodelâ€™s robustness and generalization without altering its\noriginal architecture. PoE (Hinton 2002) employs a uniqueintegration of individual expert models by multiplying their\nprobability distributions, including a biased distribution de-\nrived solely from entity inputs, with the modelâ€™s predictive\ndistribution. This multiplication and subsequent renormal-\nization subtly decrease the influence of samples with signifi-\ncant entity bias, effectively reducing the modelâ€™s reliance on\nthese entities while optimizing model performance. CoRE\n(Wang et al. 2022) mitigates biases by constructing a causal\ngraph to identify dependencies and using counterfactual sce-\nnarios to pinpoint entity biases, subsequently refining pre-\ndictions through an adaptive bias mitigation process that em-\nphasizes textual context over entity reliance, leading to de-\nbiased outcomes.\nMain results. Table 1 demonstrates the performance com-\nparison of various relation extraction models and different\ndebiasing methods on different datasets, where F1origin rep-\nresents the performance on the original test set, and F1DREB\nrepresents the performance on DREB benchmark proposed\nin this paper.\nThe experimental outcomes yield these insights: LUKE\nand IRE experienced a notable decline in performance on\nthe DREB, suggesting their initial high results were par-\ntially due to reliance on entity mentions that were either re-\nmoved or disguised in the DREB context, thereby affecting\ntheir efficacy. Focal and R-Drop , though not originally in-\ntended to tackle entity bias, have still been found to allevi-\nate it. These techniques, primarily targeting overfitting, in-\ncidentally lessen the modelsâ€™ dependency on entity cues, in-\ndicating that generalization-focused strategies can also indi-\nrectly benefit bias reduction. DFL and PoE , as targeted de-\nbiasing approaches, markedly bolstered model performance\non DREB through the incorporation of bias evaluation and\nadjustment within the training regime. However, this en-\nhancement seems to have compromised the modelsâ€™ per-\nformance on the original data. CoRE , tailored to counter-\nact entity bias, successfully improved DREB performance\nwithout sacrificing the original datasetâ€™s results, reflecting a\nbalanced and potent debiasing approach. In sum, our pro-\nposed MixDebias method has impressively uplifted perfor-\nmance on DREB while also maintaining or even enhanc-\ning the original datasetâ€™s performance, showcasing its robust\nadaptability and debiasing capabilities.ModelTACRED TACREV Re-TACRED\nF1origin F1DREB BME F1origin F1DREB BME F1origin F1DREB BME\nLUKE 70.82 44.40 - 80.16 50.60 - 88.92 39.40 -\n+Focal 69.94 45.55 1.01 79.15 52.48 1.01 88.58 39.32 1.00\n+R-Drop 70.99 46.68 1.03 81.06 53.85 1.04 89.53 40.89 1.02\n+DFL 65.04 48.48 1.01 71.31 53.17 0.97 84.15 43.94 1.03\n+PoE 63.32 47.63 0.98 68.82 52.02 0.94 82.46 44.10 1.02\n+CoRE 70.04 47.87 1.03 79.82 54.88 1.04 87.13 41.94 1.02\n+MixDebias 69.93 62.44 1.20 80.91 72.93 1.23 87.95 77.71 1.48\nIRE 71.27 50.94 - 79.36 57.20 - 87.43 46.25 -\n+Focal 71.11 50.97 1.00 78.55 57.51 1.00 87.51 48.22 1.02\n+R-Drop 71.13 52.98 1.02 80.37 59.71 1.03 87.96 48.40 1.03\n+DFL 65.72 56.28 1.01 70.18 60.13 0.97 80.17 54.03 1.04\n+PoE 64.72 54.67 0.99 69.12 59.28 0.95 81.35 51.41 1.02\n+CoRE 70.43 55.00 1.03 78.82 60.81 1.03 86.21 48.36 1.02\n+MixDebias 71.99 70.02 1.19 80.97 79.15 1.20 87.27 82.17 1.39\nTable 1: The overall evaluation results. MixDebias significantly enhances performance on DREB and achieves comparable\nperformance to the best models on the original dataset. In terms of the comprehensive metric BME, MixDebias also leads far\nahead of other baseline methods.\nModelTACRED TACREV Re-TACRED\nF1origin F1DREB F1origin F1DREB F1origin F1DREB\nLUKE+MixDebias 69.93 62.44 80.91 72.93 87.95 77.71\n-CDA 69.66(-0.27) 62.06(-0.38) 80.63(-0.28) 71.99(-0.94) 88.45(+0.50) 78.26(+0.55)\n-RDA 69.68(-0.25) 45.77(-16.67) 79.32(-1.59) 51.91(-21.02) 88.69(+0.74) 39.72(-37.99)\nIRE+MixDebias 71.99 70.02 80.97 79.15 87.27 82.17\n-CDA 71.92(-0.07) 70.21(+0.19) 80.78(-0.19) 78.60(-0.55) 87.19(-0.08) 82.08(-0.09)\n-RDA 71.33(-0.66) 52.60(-17.42) 79.36(-1.61) 58.48(-20.67) 87.87(+0.60) 48.22(-33.95)\nTable 2: The ablation study results for the MixDebias method, detailing the performance impacts of individual components\nCDA and RDA.\nAblation study. As shown in Table 2, we conducted an\nablation study on the two components of MixDebias, RDA\nand CDA. From the experimental results, we can draw the\nfollowing conclusions: Both RDA and CDA are effective\nmethods for removing entity bias. Overall, RDA is more ef-\nfective than CDA. However, in most scenarios, these two\nmethods are complementary and can enhance performance\non DREB while minimizing the impact on the performance\nof the original dataset.\nAt the same time, we conducted a more detailed abla-\ntion analysis on the hyperparameters Î²andÎ»in MixDe-\nbias. Here, Î²represents the weight of the KL divergence,\nwith a value range of [0.0, 1.0]; and Î»represents the hy-\nperparameter for estimating the biased probability distribu-\ntion of samples using causal effects, with a value range of\n[-0.6, 0.6]. From Figure 6, we can draw the following con-\nclusions: When Î²= 0, it is equivalent to the model not\nconsidering RDA. However, when Î²Ì¸= 0, introducing RDA\nleads to significant performance improvements, and as Î²in-\ncreases, the debiasing effect becomes stronger. Particularly\non noisy datasets such as TACRED and TACREV , the modelalso shows a slight performance improvement on the origi-\nnal dataset. Compared to Î², theÎ»parameter has a smaller\nimpact on model performance. When Î»= 0.2, the model\nperforms optimally. This suggests that after applying the\nRDA method, the level of entity bias in the samples is al-\nready significantly reduced. In this case, CDA mainly ad-\ndresses the bias that is difficult to correct at the data level,\nserving as a complementary effect to RDA, thereby further\nreducing the modelâ€™s reliance on entities.\nModel generalization analysis. As shown in Figure 7,\nwe plotted the label probability distribution of the model\nunder the setting of entity-only input in the original test\nset before and after debiasing on the TACRED, TACREV ,\nand Re-TACRED datasets. The experimental results show\nthat for the baseline relation extraction model, under the\nentity-only input setting, the label probabilities are primar-\nily concentrated around values close to 1, indicating that\nentity mentions significantly influences the modelâ€™s predic-\ntion outcomes. After applying MixDebias debiasing method,\nthe output probabilities of the model become notably more\nuniform. At this point, the pseudo-correlation between en-TACRED(F1)TACREV(F1)Re-TACRED(F1)\nOriginDREBOriginDREBOriginDREB\nOriginDREBOriginDREBOriginDREBFigure 6: The detailed ablation analysis on the hyperparameters Î»andÎ²in MixDebias.\nw/o MixDebiasw/ MixDebiasw/o MixDebiasw/ MixDebiasw/o MixDebiasw/ MixDebiasTACREDTACREVRe-TACREDRelative FrequencyProbability under theEntity-only Setting\nFigure 7: The visualization of debiasing effect. The substantial reduction in model reliance on entity mentions with MixDebias\nleads to a more uniform probability distribution.\ntity mentions and relation types is significantly reduced,\ndecreasing the likelihood of the model making incorrect\npredictions due to entity misguidance, thus enhancing the\nmodelâ€™s generalization capability.\nConclusion\nThis paper introduces DREB, a debiased relation extraction\nbenchmark, and MixDebias, a novel debiasing method that\naddresses entity bias in relation extraction models. DREBâ€™s\nstrength lies in its ability to sever spurious links between en-\ntity mentions and relation types through strategic entity re-placement, fostering a benchmark with diminished bias and\nelevated naturalness. This is achieved with Bias Evaluator\nand PPL Evaluator, which ensure the benchmark maintains\na high standard of impartiality and linguistic authenticity.\nMixDebias enhances model performance on DREB while\nmaintaining robustness on the original dataset through a\ncombination of data-level augmentation and model-level de-\nbiasing strategies. Comprehensive experiments demonstrate\nMixDebiasâ€™s effectiveness in improving model generaliza-\ntion and reducing reliance on entity mentions, setting a new\nstandard for debiasing in relation extraction tasks.References\nAlt, C.; Gabryszak, A.; and Hennig, L. 2020. TACRED Re-\nvisited: A Thorough Evaluation of the TACRED Relation\nExtraction Task. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics , 1558â€“\n1569.\nBelov, D. I.; and Armstrong, R. D. 2011. Distributions of\nthe Kullbackâ€“Leibler divergence with applications. British\nJournal of Mathematical and Statistical Psychology , 64(2):\n291â€“309.\nBowman, S.; Angeli, G.; Potts, C.; and Manning, C. D. 2015.\nA large annotated corpus for learning natural language infer-\nence. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , 632â€“642.\nGururangan, S.; Swayamdipta, S.; Levy, O.; Schwartz, R.;\nBowman, S.; and Smith, N. A. 2018. Annotation Artifacts\nin Natural Language Inference Data. In Proceedings of the\n2018 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 2 (Short Papers) , 107â€“112.\nHendrickx, I.; Kim, S. N.; Kozareva, Z.; Nakov, P.;\nSÂ´eaghdha, D. Â´O.; Pad Â´o, S.; Pennacchiotti, M.; Romano, L.;\nand Szpakowicz, S. 2010. SemEval-2010 Task 8: Multi-Way\nClassification of Semantic Relations between Pairs of Nom-\ninals. In Proceedings of the 5th International Workshop on\nSemantic Evaluation , 33â€“38.\nHinton, G. E. 2002. Training products of experts by mini-\nmizing contrastive divergence. Neural computation , 14(8):\n1771â€“1800.\nLiang, X.; Wu, L.; Li, J.; Wang, Y .; Meng, Q.; Qin, T.; Chen,\nW.; Zhang, M.; and Liu, T.-Y . 2021. R-Drop: regularized\ndropout for neural networks. In Proceedings of the 35th In-\nternational Conference on Neural Information Processing\nSystems , 10890â€“10905.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll Â´ar, P.\n2018. Focal Loss for Dense Object Detection. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence , 42(2):\n318â€“327.\nLongpre, S.; Perisetla, K.; Chen, A.; Ramesh, N.; DuBois,\nC.; and Singh, S. 2021. Entity-Based Knowledge Conflicts\nin Question Answering. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 7052â€“7063.\nMahabadi, R. K.; Belinkov, Y .; and Henderson, J. 2020.\nEnd-to-End Bias Mitigation by Modelling Biases in Cor-\npora. In Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics , 8706â€“8716.\nMcCoy, T.; Pavlick, E.; and Linzen, T. 2019. Right for the\nWrong Reasons: Diagnosing Syntactic Heuristics in Natu-\nral Language Inference. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics ,\n3428â€“3448.\nPeng, H.; Gao, T.; Han, X.; Lin, Y .; Li, P.; Liu, Z.; Sun, M.;\nand Zhou, J. 2020. Learning from Context or Names? An\nEmpirical Study on Neural Relation Extraction. In Proceed-\nings of the 2020 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP) , 3661â€“3672.Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nSutskever, I.; et al. 2019. Language models are unsupervised\nmultitask learners. OpenAI blog , 1(8): 9.\nReimers, N.; and Gurevych, I. 2019. Sentence-BERT:\nSentence Embeddings using Siamese BERT-Networks. In\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing\n(EMNLP-IJCNLP) , 3982â€“3992.\nSchuster, T.; Shah, D.; Yeo, Y . J. S.; Ortiz, D. R. F.; San-\ntus, E.; and Barzilay, R. 2019. Towards Debiasing Fact Ver-\nification Models. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP) , 3419â€“3425.\nSmith, L. I. 2002. A tutorial on principal components anal-\nysis.\nStoica, G.; Platanios, E. A.; and P Â´oczos, B. 2021. Re-tacred:\nAddressing shortcomings of the tacred dataset. In Proceed-\nings of the AAAI conference on artificial intelligence , vol-\nume 35, 13843â€“13850.\nVrande Ë‡ciÂ´c, D.; and Kr Â¨otzsch, M. 2014. Wikidata: a free\ncollaborative knowledgebase. Communications of the ACM ,\n57(10): 78â€“85.\nWang, F.; Mo, W.; Wang, Y .; Zhou, W.; and Chen, M. 2023a.\nA Causal View of Entity Bias in (Large) Language Models.\nInFindings of the Association for Computational Linguis-\ntics: EMNLP 2023 , 15173â€“15184.\nWang, Y .; Chen, M.; Zhou, W.; Cai, Y .; Liang, Y .; Liu, D.;\nYang, B.; Liu, J.; and Hooi, B. 2022. Should We Rely on En-\ntity Mentions for Relation Extraction? Debiasing Relation\nExtraction with Counterfactual Analysis. In Proceedings\nof the 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Lan-\nguage Technologies , 3071â€“3081.\nWang, Y .; Hooi, B.; Wang, F.; Cai, Y .; Liang, Y .; Zhou, W.;\nTang, J.; Duan, M.; and Chen, M. 2023b. How Fragile is\nRelation Extraction under Entity Replacements? In Pro-\nceedings of the 27th Conference on Computational Natural\nLanguage Learning (CoNLL) , 414â€“423.\nWilliams, A.; Nangia, N.; and Bowman, S. 2018. A Broad-\nCoverage Challenge Corpus for Sentence Understanding\nthrough Inference. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Vol-\nume 1 (Long Papers) , 1112â€“1122.\nYamada, I.; Asai, A.; Shindo, H.; Takeda, H.; and Mat-\nsumoto, Y . 2020. LUKE: Deep Contextualized Entity Repre-\nsentations with Entity-aware Self-attention. In Proceedings\nof the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , 6442â€“6454.\nZhang, Y .; Qi, P.; and Manning, C. D. 2018. Graph Convolu-\ntion over Pruned Dependency Trees Improves Relation Ex-\ntraction. In Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing , 2205â€“2215.\nZhang, Y .; Zhong, V .; Chen, D.; Angeli, G.; and Manning,\nC. D. 2017. Position-aware Attention and Supervised DataImprove Slot Filling. In Proceedings of the 2017 Conference\non Empirical Methods in Natural Language Processing , 35â€“\n45.\nZhou, W.; and Chen, M. 2022. An Improved Baseline for\nSentence-level Relation Extraction. In Proceedings of the\n2nd Conference of the Asia-Pacific Chapter of the Associ-\nation for Computational Linguistics and the 12th Interna-\ntional Joint Conference on Natural Language Processing\n(Volume 2: Short Papers) , 161â€“168.
\.


--
-- TOC entry 3372 (class 0 OID 16393)
-- Dependencies: 210
-- Data for Name: project_folders; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.project_folders (folder_id, project_id, parent_folder_id, name, created_at, path_array, updated_at) FROM stdin;
2abb152c-1399-4e6d-b9d0-ee5dc4c44528	187b38e9-506e-44c5-bc8a-642731c6122b	\N	Documents	2025-01-10 16:47:05.544098-05	{187b38e9-506e-44c5-bc8a-642731c6122b}	2025-01-15 06:58:06.917966-05
08e789c2-74e5-4cac-93d5-26fe328a26b6	187b38e9-506e-44c5-bc8a-642731c6122b	\N	Reports	2025-01-10 16:47:05.544098-05	{187b38e9-506e-44c5-bc8a-642731c6122b}	2025-01-15 06:58:06.917966-05
83add662-ea64-467f-a2a9-3b4a36fea74b	187b38e9-506e-44c5-bc8a-642731c6122b	\N	Research Notes	2025-01-10 16:47:05.544098-05	{187b38e9-506e-44c5-bc8a-642731c6122b}	2025-01-15 06:58:06.917966-05
e28b52e0-380f-471c-bc89-b804a7ad5189	a2e2e85f-af19-47b2-b5ab-7b7815243751	\N	Documents	2025-01-10 16:48:41.315949-05	{a2e2e85f-af19-47b2-b5ab-7b7815243751}	2025-01-15 06:58:06.917966-05
9f5b84ce-262e-457e-ab29-a4e78e6fd839	a2e2e85f-af19-47b2-b5ab-7b7815243751	\N	Reports	2025-01-10 16:48:41.315949-05	{a2e2e85f-af19-47b2-b5ab-7b7815243751}	2025-01-15 06:58:06.917966-05
64c50486-b3c2-42e0-922e-b29ed85b58a0	a2e2e85f-af19-47b2-b5ab-7b7815243751	\N	Research Notes	2025-01-10 16:48:41.315949-05	{a2e2e85f-af19-47b2-b5ab-7b7815243751}	2025-01-15 06:58:06.917966-05
1572e013-97f3-4dc6-b860-572cf420f8dc	24bf464a-cf5d-48ce-be2b-6e46f38bf5ac	\N	Documents	2025-01-10 16:49:45.176987-05	{24bf464a-cf5d-48ce-be2b-6e46f38bf5ac}	2025-01-15 06:58:06.917966-05
15363bb4-5794-4d9c-b2bf-eb1190ecb7a5	64f68f62-7490-4757-ae38-6cadac346cc9	\N	Documents	2025-01-10 16:56:31.503011-05	{64f68f62-7490-4757-ae38-6cadac346cc9}	2025-01-15 06:58:06.917966-05
cca4d853-e4e4-42f0-b85e-26e9206c9796	64f68f62-7490-4757-ae38-6cadac346cc9	\N	Reports	2025-01-10 16:56:31.503011-05	{64f68f62-7490-4757-ae38-6cadac346cc9}	2025-01-15 06:58:06.917966-05
3a37eb8c-e6ad-42fb-8f2c-be640ecd5260	64f68f62-7490-4757-ae38-6cadac346cc9	\N	Research Notes	2025-01-10 16:56:31.503011-05	{64f68f62-7490-4757-ae38-6cadac346cc9}	2025-01-15 06:58:06.917966-05
49322935-f6cd-4dba-8887-5b164eb8b0dd	12b06f14-95c6-4610-a7b5-c660a5b4daa3	\N	Documents	2025-01-10 17:00:28.381984-05	{12b06f14-95c6-4610-a7b5-c660a5b4daa3}	2025-01-15 06:58:06.917966-05
234a4129-d99a-446f-8ef1-c2fda8105b3c	12b06f14-95c6-4610-a7b5-c660a5b4daa3	\N	Reports	2025-01-10 17:00:28.381984-05	{12b06f14-95c6-4610-a7b5-c660a5b4daa3}	2025-01-15 06:58:06.917966-05
34761a93-c0af-488a-a921-0778507afb3a	12b06f14-95c6-4610-a7b5-c660a5b4daa3	\N	Research Notes	2025-01-10 17:00:28.381984-05	{12b06f14-95c6-4610-a7b5-c660a5b4daa3}	2025-01-15 06:58:06.917966-05
\.


--
-- TOC entry 3371 (class 0 OID 16386)
-- Dependencies: 209
-- Data for Name: research_projects; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.research_projects (project_id, name, description, created_at, updated_at, owner_id, status, settings) FROM stdin;
24bf464a-cf5d-48ce-be2b-6e46f38bf5ac	test_osint	superbigtest	\N	2025-01-10 16:49:45.175743-05	aa5c38ff-7fb4-41d0-9fb3-ed2d67d3b4c3	active	{}
64f68f62-7490-4757-ae38-6cadac346cc9	test_osint	superbigtest	\N	2025-01-10 16:56:31.501776-05	aa5c38ff-7fb4-41d0-9fb3-ed2d67d3b4c3	active	{}
12b06f14-95c6-4610-a7b5-c660a5b4daa3	cool	stuff	2025-01-10 17:00:28.381984-05	2025-01-10 17:00:28.381984-05	aa5c38ff-7fb4-41d0-9fb3-ed2d67d3b4c3	active	{}
a2e2e85f-af19-47b2-b5ab-7b7815243751	test_osint	superbig	\N	2025-01-11 13:45:46.861614-05	aa5c38ff-7fb4-41d0-9fb3-ed2d67d3b4c3	active	{}
187b38e9-506e-44c5-bc8a-642731c6122b	test_osint	super	\N	2025-01-11 13:46:05.877422-05	aa5c38ff-7fb4-41d0-9fb3-ed2d67d3b4c3	active	{}
c12ce2ef-d938-4b78-a2c1-9b6ac5091e93	wunderbar	greatbigproject	\N	2025-01-11 13:52:06.689055-05	aa5c38ff-7fb4-41d0-9fb3-ed2d67d3b4c3	active	{}
\.


--
-- TOC entry 3374 (class 0 OID 16422)
-- Dependencies: 212
-- Data for Name: users; Type: TABLE DATA; Schema: public; Owner: postgres
--

COPY public.users (user_id, email, password_hash, created_at, updated_at, openai_api_key) FROM stdin;
aa5c38ff-7fb4-41d0-9fb3-ed2d67d3b4c3	test@test.com	$2b$12$2OvAEIJ0kjsJO1KT5vk3kuSl/tnalZyWz7SNWWO1Zgn/Q5O8.9Mhu	2025-01-07 17:55:30.103897	2025-01-10 14:52:23.049623	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
\.


--
-- TOC entry 3224 (class 2606 OID 16416)
-- Name: documents documents_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT documents_pkey PRIMARY KEY (document_id);


--
-- TOC entry 3222 (class 2606 OID 16399)
-- Name: project_folders project_folders_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.project_folders
    ADD CONSTRAINT project_folders_pkey PRIMARY KEY (folder_id);


--
-- TOC entry 3220 (class 2606 OID 16392)
-- Name: research_projects research_projects_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.research_projects
    ADD CONSTRAINT research_projects_pkey PRIMARY KEY (project_id);


--
-- TOC entry 3226 (class 2606 OID 16432)
-- Name: users users_email_key; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users
    ADD CONSTRAINT users_email_key UNIQUE (email);


--
-- TOC entry 3228 (class 2606 OID 16430)
-- Name: users users_pkey; Type: CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.users
    ADD CONSTRAINT users_pkey PRIMARY KEY (user_id);


--
-- TOC entry 3231 (class 2606 OID 16417)
-- Name: documents documents_folder_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.documents
    ADD CONSTRAINT documents_folder_id_fkey FOREIGN KEY (folder_id) REFERENCES public.project_folders(folder_id) ON DELETE CASCADE;


--
-- TOC entry 3229 (class 2606 OID 16405)
-- Name: project_folders project_folders_parent_folder_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.project_folders
    ADD CONSTRAINT project_folders_parent_folder_id_fkey FOREIGN KEY (parent_folder_id) REFERENCES public.project_folders(folder_id) ON DELETE SET NULL;


--
-- TOC entry 3230 (class 2606 OID 16400)
-- Name: project_folders project_folders_project_id_fkey; Type: FK CONSTRAINT; Schema: public; Owner: postgres
--

ALTER TABLE ONLY public.project_folders
    ADD CONSTRAINT project_folders_project_id_fkey FOREIGN KEY (project_id) REFERENCES public.research_projects(project_id) ON DELETE CASCADE;


--
-- TOC entry 3380 (class 0 OID 0)
-- Dependencies: 4
-- Name: SCHEMA public; Type: ACL; Schema: -; Owner: postgres
--

REVOKE USAGE ON SCHEMA public FROM PUBLIC;
GRANT ALL ON SCHEMA public TO PUBLIC;


-- Completed on 2025-01-19 18:00:14 EST

--
-- PostgreSQL database dump complete
--

